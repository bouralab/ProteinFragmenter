{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Helix Capping Residues #\n",
    "\n",
    "The goal is to identify residues just before an alpha helix begins or the residues just after the helix ends. This will improve secondary structure predictors becuase they often extend too far or do not start at the right place. \n",
    "\n",
    "The CapsDB has annoted sequences of structures of helix capping residues that can be used to train a deep nueral net. We will use a Bidirectional LSTM using phi/psi features to see if it will those will be good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Features ##\n",
    "### MMTF Pyspark Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.webfilters import Pisces\n",
    "from mmtfPyspark.filters import ContainsLProteinChain\n",
    "from mmtfPyspark.mappers import StructureToPolymerChains\n",
    "from mmtfPyspark.ml import ProteinSequenceEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/ProteinFragmenter'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import secondaryStructureExtractorFull\n",
    "#import mmtfToASA\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Context ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"DeepCap\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out chains not in CapsDB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, array_contains\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "capsdb = sqlContext.read.parquet('caps_descriptors.parquet')\n",
    "capsdb_pdbs = capsdb.select(concat(upper(col(\"pdbId\")), lit(\".\"), col(\"chain\")).alias(\"id\")).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6749"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capsdb_pdbs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import BooleanType\n",
    "\n",
    "# def hasHelixCapInfo(enc_obj, ids):\n",
    "#     return(enc_obj[0] in ids)\n",
    "\n",
    "# def udf_hasHelixCapInfo(ids):\n",
    "#     return udf(lambda t: hasHelixCapInfo(t, ids))\n",
    "#     # I'm not sure how to call these successfully - Sean\n",
    "\n",
    "capsdb_ids = set()\n",
    "[capsdb_ids.add(list[0]) for list in capsdb_pdbs.select(\"id\").collect()]\n",
    "\n",
    "class HasHelixCapInfo(object):\n",
    "    '''This filter returns true if the structure is in CAPSDB'''\n",
    "    def __call__(self, t):\n",
    "        return(t[0] in capsdb_ids)\n",
    "    # This works but I don't like that it refers to a global variable: not sure how to change that right now - Sean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MMTF File and get a set of L-protein chains ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"full\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/full.tar && tar -xvf full.tar\n",
    "        \n",
    "if not os.path.isdir(\"reduced\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/reduced.tar && tar -xvf reduced.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mmtfReader.read_sequence_file('full') \\\n",
    "    .flatMap(StructureToPolymerChains()) \\\n",
    "    .filter(ContainsLProteinChain()) \\\n",
    "    .filter(HasHelixCapInfo())\n",
    "#pdb.count()\n",
    "#pdb.take(2)\n",
    "#?pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Torsion angle and secondary structure info ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "MAPPED\n"
     ]
    }
   ],
   "source": [
    "#from mmtfPyspark.datasets import secondaryStructureExtractor\n",
    "\n",
    "data = secondaryStructureExtractorFull.get_dataset(pdb).toPandas()\n",
    "#data = secondaryStructureExtractor.get_dataset(pdb).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbId</th>\n",
       "      <th>chain</th>\n",
       "      <th>resi</th>\n",
       "      <th>resn</th>\n",
       "      <th>phi</th>\n",
       "      <th>psi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>ASP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>TYR</td>\n",
       "      <td>-151.753433</td>\n",
       "      <td>176.059341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>GLU</td>\n",
       "      <td>-114.129326</td>\n",
       "      <td>124.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>ILE</td>\n",
       "      <td>-122.864243</td>\n",
       "      <td>125.844177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>LEU</td>\n",
       "      <td>-114.774071</td>\n",
       "      <td>127.154877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>PHE</td>\n",
       "      <td>-111.492783</td>\n",
       "      <td>134.083374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>SER</td>\n",
       "      <td>-129.723282</td>\n",
       "      <td>133.550552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>8</td>\n",
       "      <td>ASP</td>\n",
       "      <td>-75.892632</td>\n",
       "      <td>-8.814808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>9</td>\n",
       "      <td>GLU</td>\n",
       "      <td>-70.092163</td>\n",
       "      <td>131.786179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "      <td>THR</td>\n",
       "      <td>-103.563698</td>\n",
       "      <td>151.883087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbId chain  resi resn         phi         psi\n",
       "0  1byf     A     1  ASP         NaN  142.984375\n",
       "1  1byf     A     2  TYR -151.753433  176.059341\n",
       "2  1byf     A     3  GLU -114.129326  124.031487\n",
       "3  1byf     A     4  ILE -122.864243  125.844177\n",
       "4  1byf     A     5  LEU -114.774071  127.154877\n",
       "5  1byf     A     6  PHE -111.492783  134.083374\n",
       "6  1byf     A     7  SER -129.723282  133.550552\n",
       "7  1byf     A     8  ASP  -75.892632   -8.814808\n",
       "8  1byf     A     9  GLU  -70.092163  131.786179\n",
       "9  1byf     A    10  THR -103.563698  151.883087"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = capsdb.toPandas()\n",
    "df = pd.merge(data, df1, left_on=('pdbId','chain'), right_on=('pdbid','chain'), how='inner')\n",
    "df = df[['pdbId', 'chain', 'resi', 'resn', 'phi', 'psi', 'startcap', 'endcap']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_cap'] = df.apply(lambda x: 1 if (x['resi'] >= x['startcap'] and x['resi'] <= x['endcap']) else 0, axis=1)\n",
    "df_caps = df.groupby([\"pdbId\", \"chain\", \"resi\"])['is_cap'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caps = pd.merge(data, df_caps, left_on=('pdbId','chain', 'resi'), right_on=('pdbId','chain', 'resi'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cap(pdbId, chain, resi, is_cap):\n",
    "    if is_cap == 1:\n",
    "        return([1,0])\n",
    "    else:\n",
    "        return([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data_caps.groupby([\"pdbId\", \"chain\"])\n",
    "                           # num pdbs,    max len of seqs, num features\n",
    "training_data = np.zeros((groups.ngroups, 5000, 2), dtype=float)\n",
    "                           # num pdbs,    max len of seqs, length of one-hot encoded target\n",
    "truth = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n",
    "truth_lagged = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbId</th>\n",
       "      <th>chain</th>\n",
       "      <th>resi</th>\n",
       "      <th>resn</th>\n",
       "      <th>phi</th>\n",
       "      <th>psi</th>\n",
       "      <th>is_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>19</td>\n",
       "      <td>HIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-56.920506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>20</td>\n",
       "      <td>MET</td>\n",
       "      <td>-75.926338</td>\n",
       "      <td>-23.480333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>21</td>\n",
       "      <td>THR</td>\n",
       "      <td>-55.579212</td>\n",
       "      <td>-47.003891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>22</td>\n",
       "      <td>GLU</td>\n",
       "      <td>-65.051697</td>\n",
       "      <td>-42.223095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>23</td>\n",
       "      <td>VAL</td>\n",
       "      <td>-60.412868</td>\n",
       "      <td>-54.407536</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbId chain  resi resn        phi        psi  is_cap\n",
       "0  4nec     A    19  HIS        NaN -56.920506       0\n",
       "1  4nec     A    20  MET -75.926338 -23.480333       0\n",
       "2  4nec     A    21  THR -55.579212 -47.003891       0\n",
       "3  4nec     A    22  GLU -65.051697 -42.223095       0\n",
       "4  4nec     A    23  VAL -60.412868 -54.407536       0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_caps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count=0\n",
    "for i, ((pdbid, chain), group) in enumerate(groups):\n",
    "    for j, featuretuple in enumerate(group.itertuples()):\n",
    "        if j>=5000: break\n",
    "        #print(featuretuple)\n",
    "        #print(count)\n",
    "        #count =count+1\n",
    "        training_data[i,j,:] = (featuretuple.phi, featuretuple.psi)\n",
    "        truth[i,j,:] = is_cap(featuretuple.pdbId, featuretuple.chain, featuretuple.resi, is_cap)#, featuretuple.startcap, featuretuple.endcap)\n",
    "        truth_lagged[i,j+1,:] = truth[i,j,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6714, 5000, 2)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_data[10]\n",
    "#truth.shape\n",
    "training_data.shape\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "os.remove('features.h5')\n",
    "os.remove('truth.h5')\n",
    "os.remove('truthlag.h5')\n",
    "\n",
    "h5f = h5py.File('features.h5', 'w')\n",
    "h5f.create_dataset('training_data', data=training_data)\n",
    "h5f.close()\n",
    "\n",
    "h5tr = h5py.File('truth.h5', 'w')\n",
    "h5tr.create_dataset('truth', data=truth)\n",
    "h5tr.close()\n",
    "\n",
    "h5trl = h5py.File('truthlag.h5', 'w')\n",
    "h5trl.create_dataset('truthlag', data=truth_lagged)\n",
    "h5trl.close()\n",
    "\n",
    "#os.remove(\"features.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write features to H5 file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caps_pdb.write.mode('overwrite').format('hdf').save('./features.h5')\n",
    "#training_data.write.mode('overwrite').format('hdf').save('./features.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get truth labels and Save to H5 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out truth.h5..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Bidirectional LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_file, initial_learning_rate=0.0001, learning_rate_drop=0.5, learning_rate_epochs=None,\n",
    "                  learning_rate_patience=50, logging_file=\"training.log\", verbosity=1,\n",
    "                  early_stopping_patience=None):\n",
    "    callbacks = list()\n",
    "    callbacks.append(ModelCheckpoint(model_file, save_best_only=True))\n",
    "    callbacks.append(CSVLogger(logging_file, append=True))\n",
    "    if learning_rate_epochs:\n",
    "        callbacks.append(LearningRateScheduler(partial(step_decay, initial_lrate=initial_learning_rate,\n",
    "                                                       drop=learning_rate_drop, epochs_drop=learning_rate_epochs)))\n",
    "    else:\n",
    "        callbacks.append(ReduceLROnPlateau(factor=learning_rate_drop, patience=learning_rate_patience,\n",
    "                                           verbose=verbosity))\n",
    "    if early_stopping_patience:\n",
    "        callbacks.append(EarlyStopping(verbose=verbosity, patience=early_stopping_patience))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_features, num_outputs=2, latent_dim=100):\n",
    "    \"\"\"Create a Seq2Seq Bidirectional LSTM\n",
    "    From: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features : int\n",
    "        The number of features in your trianing data\n",
    "    num_outputs : int\n",
    "        Number of outputs to rpedict, i.e. number of classes or 2 for binary\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A new Keras Seq2Seq Bidirectional LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_features))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    #encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    #encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    #state_h = Concatenate()([forward_h, backward_h])\n",
    "    #state_c = Concatenate()([forward_c, backward_c])\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    #decoder_inputs = Input(shape=(None, num_outputs))\n",
    "        \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    #decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    #decoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "    #decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, num_outputs))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    #decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(num_outputs, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    #return model\n",
    "    return encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    encoder_input_data = HDF5Matrix('features.h5', 'training_data')\n",
    "    decoder_input_data = HDF5Matrix('truth.h5', 'truth')\n",
    "    decoder_target_data = HDF5Matrix('truthlag.h5', 'truthlag')\n",
    "    encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model = create_model(num_features=2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Automicatlly determine batch sizes, train/test splits\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data, shuffle=\"batch\")\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model_StS_BiLSTM.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model_StS_BiLSTM.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6714/6714 [==============================] - 3018s 450ms/step - loss: nan - acc: 0.9491\n",
      "Saved model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/myBinder-environment/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_StS_BiLSTM.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_StS_BiLSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'lstm_13/kernel:0' shape=(2, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_13/recurrent_kernel:0' shape=(100, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_13/bias:0' shape=(400,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_lstm.weights\n",
    "#model.layers[5].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do steps to prepare for inference\n",
    "latent_dim=100\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 2))\n",
    "    # Populate the first character of target sequence with the start residue.\n",
    "    target_seq[0, 0, :] = [0, 1] # decoder tokens one-hot encoding hardcoded here\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_chain = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        decoded_chain.append(sampled_token_index)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if len(decoded_chain) > 5000:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        sampled_residue = [0, 0]\n",
    "        sampled_residue[sampled_token_index] = 1\n",
    "        target_seq = np.zeros((1, 1, 2))\n",
    "        target_seq[0, 0, :] = sampled_residue\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/myBinder-environment/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#training_data = HDF5Matrix('features.h5', 'training_data')\n",
    "import h5py\n",
    "hf = h5py.File('features.h5', 'r')\n",
    "df_training = hf.get('training_data').value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6714, 5000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_training[4:5,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = decode_sequence(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for j in result:\n",
    "    i += j\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jvm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ad009ee88bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/myBinder-environment/lib/python3.6/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jvm'"
     ]
    }
   ],
   "source": [
    "sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mybinder-environment",
   "language": "python",
   "name": "conda_mybinder-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
