{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Helix Capping Residues #\n",
    "\n",
    "The goal is to identify residues just before an alpha helix begins or the residues just after the helix ends. This will improve secondary structure predictors becuase they often extend too far or do not start at the right place. \n",
    "\n",
    "The CapsDB has annoted sequences of structures of helix capping residues that can be used to train a deep nueral net. We will use a Bidirectional LSTM using phi/psi features to see if it will those will be good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Features ##\n",
    "### MMTF Pyspark Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.webfilters import Pisces\n",
    "from mmtfPyspark.filters import ContainsLProteinChain\n",
    "from mmtfPyspark.mappers import StructureToPolymerChains\n",
    "from mmtfPyspark.ml import ProteinSequenceEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/ProteinFragmenter'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import secondaryStructureExtractorFull\n",
    "#import mmtfToASA\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Context ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"DeepCap\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out chains not in CapsDB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, array_contains\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "capsdb = sqlContext.read.parquet('caps_descriptors.parquet')\n",
    "capsdb_pdbs = capsdb.select(concat(upper(col(\"pdbId\")), lit(\".\"), col(\"chain\")).alias(\"id\")).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6749"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capsdb_pdbs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import BooleanType\n",
    "\n",
    "# def hasHelixCapInfo(enc_obj, ids):\n",
    "#     return(enc_obj[0] in ids)\n",
    "\n",
    "# def udf_hasHelixCapInfo(ids):\n",
    "#     return udf(lambda t: hasHelixCapInfo(t, ids))\n",
    "#     # I'm not sure how to call these successfully - Sean\n",
    "\n",
    "capsdb_ids = set()\n",
    "[capsdb_ids.add(list[0]) for list in capsdb_pdbs.select(\"id\").collect()]\n",
    "\n",
    "class HasHelixCapInfo(object):\n",
    "    '''This filter returns true if the structure is in CAPSDB'''\n",
    "    def __call__(self, t):\n",
    "        return(t[0] in capsdb_ids)\n",
    "    # This works but I don't like that it refers to a global variable: not sure how to change that right now - Sean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MMTF File and get a set of L-protein chains ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"full\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/full.tar && tar -xvf full.tar\n",
    "        \n",
    "if not os.path.isdir(\"reduced\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/reduced.tar && tar -xvf reduced.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mmtfReader.read_sequence_file('full') \\\n",
    "    .flatMap(StructureToPolymerChains()) \\\n",
    "    .filter(ContainsLProteinChain()) \\\n",
    "    .filter(HasHelixCapInfo())\n",
    "#pdb.count()\n",
    "#pdb.take(2)\n",
    "#?pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Torsion angle and secondary structure info ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "MAPPED\n"
     ]
    }
   ],
   "source": [
    "#from mmtfPyspark.datasets import secondaryStructureExtractor\n",
    "\n",
    "data = secondaryStructureExtractorFull.get_dataset(pdb).toPandas()\n",
    "#data = secondaryStructureExtractor.get_dataset(pdb).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbId</th>\n",
       "      <th>chain</th>\n",
       "      <th>resi</th>\n",
       "      <th>resn</th>\n",
       "      <th>phi</th>\n",
       "      <th>psi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>ASP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>TYR</td>\n",
       "      <td>-151.753433</td>\n",
       "      <td>176.059341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>GLU</td>\n",
       "      <td>-114.129326</td>\n",
       "      <td>124.031487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>ILE</td>\n",
       "      <td>-122.864243</td>\n",
       "      <td>125.844177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>LEU</td>\n",
       "      <td>-114.774071</td>\n",
       "      <td>127.154877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>PHE</td>\n",
       "      <td>-111.492783</td>\n",
       "      <td>134.083374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>SER</td>\n",
       "      <td>-129.723282</td>\n",
       "      <td>133.550552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>8</td>\n",
       "      <td>ASP</td>\n",
       "      <td>-75.892632</td>\n",
       "      <td>-8.814808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>9</td>\n",
       "      <td>GLU</td>\n",
       "      <td>-70.092163</td>\n",
       "      <td>131.786179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1byf</td>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "      <td>THR</td>\n",
       "      <td>-103.563698</td>\n",
       "      <td>151.883087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbId chain  resi resn         phi         psi\n",
       "0  1byf     A     1  ASP         NaN  142.984375\n",
       "1  1byf     A     2  TYR -151.753433  176.059341\n",
       "2  1byf     A     3  GLU -114.129326  124.031487\n",
       "3  1byf     A     4  ILE -122.864243  125.844177\n",
       "4  1byf     A     5  LEU -114.774071  127.154877\n",
       "5  1byf     A     6  PHE -111.492783  134.083374\n",
       "6  1byf     A     7  SER -129.723282  133.550552\n",
       "7  1byf     A     8  ASP  -75.892632   -8.814808\n",
       "8  1byf     A     9  GLU  -70.092163  131.786179\n",
       "9  1byf     A    10  THR -103.563698  151.883087"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = capsdb.toPandas()\n",
    "df = pd.merge(data, df1, left_on=('pdbId','chain'), right_on=('pdbid','chain'), how='inner')\n",
    "df = df[['pdbId', 'chain', 'resi', 'resn', 'phi', 'psi', 'startcap', 'endcap']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_cap'] = df.apply(lambda x: 1 if (x['resi'] >= x['startcap'] and x['resi'] <= x['endcap']) else 0, axis=1)\n",
    "df_caps = df.groupby([\"pdbId\", \"chain\", \"resi\"])['is_cap'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caps = pd.merge(data, df_caps, left_on=('pdbId','chain', 'resi'), right_on=('pdbId','chain', 'resi'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cap(pdbId, chain, resi, is_cap):\n",
    "    if is_cap == 1:\n",
    "        return([1,0])\n",
    "    else:\n",
    "        return([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data_caps.groupby([\"pdbId\", \"chain\"])\n",
    "                           # num pdbs,    max len of seqs, num features\n",
    "training_data = np.zeros((groups.ngroups, 5000, 2), dtype=float)\n",
    "                           # num pdbs,    max len of seqs, length of one-hot encoded target\n",
    "truth = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n",
    "truth_lagged = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbId</th>\n",
       "      <th>chain</th>\n",
       "      <th>resi</th>\n",
       "      <th>resn</th>\n",
       "      <th>phi</th>\n",
       "      <th>psi</th>\n",
       "      <th>is_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>19</td>\n",
       "      <td>HIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-56.920506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>20</td>\n",
       "      <td>MET</td>\n",
       "      <td>-75.926338</td>\n",
       "      <td>-23.480333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>21</td>\n",
       "      <td>THR</td>\n",
       "      <td>-55.579212</td>\n",
       "      <td>-47.003891</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>22</td>\n",
       "      <td>GLU</td>\n",
       "      <td>-65.051697</td>\n",
       "      <td>-42.223095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4nec</td>\n",
       "      <td>A</td>\n",
       "      <td>23</td>\n",
       "      <td>VAL</td>\n",
       "      <td>-60.412868</td>\n",
       "      <td>-54.407536</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbId chain  resi resn        phi        psi  is_cap\n",
       "0  4nec     A    19  HIS        NaN -56.920506       0\n",
       "1  4nec     A    20  MET -75.926338 -23.480333       0\n",
       "2  4nec     A    21  THR -55.579212 -47.003891       0\n",
       "3  4nec     A    22  GLU -65.051697 -42.223095       0\n",
       "4  4nec     A    23  VAL -60.412868 -54.407536       0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_caps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count=0\n",
    "for i, ((pdbid, chain), group) in enumerate(groups):\n",
    "    for j, featuretuple in enumerate(group.itertuples()):\n",
    "        if j>=5000: break\n",
    "        #print(featuretuple)\n",
    "        #print(count)\n",
    "        #count =count+1\n",
    "        training_data[i,j,:] = (featuretuple.phi, featuretuple.psi)\n",
    "        truth[i,j,:] = is_cap(featuretuple.pdbId, featuretuple.chain, featuretuple.resi, is_cap)#, featuretuple.startcap, featuretuple.endcap)\n",
    "        truth_lagged[i,j+1,:] = truth[i,j,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6714, 5000, 2)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_data[10]\n",
    "#truth.shape\n",
    "training_data.shape\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "os.remove('features.h5')\n",
    "os.remove('truth.h5')\n",
    "os.remove('truthlag.h5')\n",
    "\n",
    "h5f = h5py.File('features.h5', 'w')\n",
    "h5f.create_dataset('training_data', data=training_data)\n",
    "h5f.close()\n",
    "\n",
    "h5tr = h5py.File('truth.h5', 'w')\n",
    "h5tr.create_dataset('truth', data=truth)\n",
    "h5tr.close()\n",
    "\n",
    "h5trl = h5py.File('truthlag.h5', 'w')\n",
    "h5trl.create_dataset('truthlag', data=truth_lagged)\n",
    "h5trl.close()\n",
    "\n",
    "#os.remove(\"features.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write features to H5 file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caps_pdb.write.mode('overwrite').format('hdf').save('./features.h5')\n",
    "#training_data.write.mode('overwrite').format('hdf').save('./features.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get truth labels and Save to H5 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out truth.h5..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Bidirectional LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_features, num_outputs=2, latent_dim=100):\n",
    "    \"\"\"Create a Seq2Seq Bidirectional LSTM\n",
    "    From: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features : int\n",
    "        The number of features in your trianing data\n",
    "    num_outputs : int\n",
    "        Number of outputs to rpedict, i.e. number of classes or 2 for binary\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A new Keras Seq2Seq Bidirectional LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_features))\n",
    "    #encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "    #encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    #decoder_inputs = Input(shape=(None, num_outputs))\n",
    "        \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    #decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    #decoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "    #decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, num_outputs))\n",
    "    decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(num_outputs, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    encoder_input_data = HDF5Matrix('features.h5', 'training_data')\n",
    "    decoder_input_data = HDF5Matrix('truth.h5', 'truth')\n",
    "    decoder_target_data = HDF5Matrix('truthlag.h5', 'truthlag')\n",
    "    model = create_model(num_features=2)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Automicatlly determine batch sizes, train/test splits\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data, shuffle=\"batch\")\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model_StS_BiLSTM.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model_StS_BiLSTM.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6714/6714 [==============================] - 4234s 631ms/step - loss: nan - acc: 0.9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/myBinder-environment/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_5/concat:0' shape=(?, 200) dtype=float32>, <tf.Tensor 'concatenate_6/concat:0' shape=(?, 200) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_StS_BiLSTM.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_StS_BiLSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7fd43a841470>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x7fd43a9f4ef0>,\n",
       " <keras.engine.input_layer.InputLayer at 0x7fd43a841748>,\n",
       " <keras.layers.merge.Concatenate at 0x7fd43a841860>,\n",
       " <keras.layers.merge.Concatenate at 0x7fd43a841ac8>,\n",
       " <keras.layers.recurrent.LSTM at 0x7fd43a8417f0>,\n",
       " <keras.layers.core.Dense at 0x7fd43a841828>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_inputs = loaded_model.layers[0]\n",
    "temp_encoder_inputs = [loaded_model.layers[3], loaded_model.layers[4]]\n",
    "temp_inputs = loaded_model.layers[0]\n",
    "loaded_model.layers\n",
    "#?temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-bc1356c8999c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_inputs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mybinder-environment",
   "language": "python",
   "name": "conda_mybinder-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
