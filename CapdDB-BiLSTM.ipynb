{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Helix Capping Residues #\n",
    "\n",
    "The goal is to identify residues just before an alpha helix begins or the residues just after the helix ends. This will improve secondary structure predictors becuase they often extend too far or do not start at the right place. \n",
    "\n",
    "The CapsDB has annoted sequences of structures of helix capping residues that can be used to train a deep nueral net. We will use a Bidirectional LSTM using phi/psi features to see if it will those will be good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Features ##\n",
    "### MMTF Pyspark Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.webfilters import Pisces\n",
    "from mmtfPyspark.filters import ContainsLProteinChain\n",
    "from mmtfPyspark.mappers import StructureToPolymerChains\n",
    "from mmtfPyspark.ml import ProteinSequenceEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/ProteinFragmenter'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import secondaryStructureExtractorFull\n",
    "#import mmtfToASA\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Context ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"DeepCap\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out chains not in CapsDB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, array_contains\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "capsdb = sqlContext.read.parquet('caps_descriptors.parquet')\n",
    "capsdb_pdbs = capsdb.select(concat(upper(col(\"pdbId\")), lit(\".\"), col(\"chain\")).alias(\"id\")).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import BooleanType\n",
    "\n",
    "# def hasHelixCapInfo(enc_obj, ids):\n",
    "#     return(enc_obj[0] in ids)\n",
    "\n",
    "# def udf_hasHelixCapInfo(ids):\n",
    "#     return udf(lambda t: hasHelixCapInfo(t, ids))\n",
    "#     # I'm not sure how to call these successfully - Sean\n",
    "\n",
    "capsdb_ids = [list[0] for list in capsdb_pdbs.select(\"id\").collect()]\n",
    "class HasHelixCapInfo(object):\n",
    "    '''This filter returns true if the structure is in CAPSDB'''\n",
    "    def __call__(self, t):\n",
    "        return(t[0] in capsdb_ids)\n",
    "    # This works but I don't like that it refers to a global variable: not sure how to change that right now - Sean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MMTF File and get a set of L-protein chains ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"full\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/full.tar && tar -xvf full.tar\n",
    "        \n",
    "if not os.path.isdir(\"reduced\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/reduced.tar && tar -xvf reduced.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb = mmtfReader.read_sequence_file('full/part-00001') \\\n",
    "    .flatMap(StructureToPolymerChains()) \\\n",
    "    .filter(ContainsLProteinChain()) \\\n",
    "    .filter(HasHelixCapInfo())\n",
    "pdb.count()\n",
    "#pdb.take(2)\n",
    "#?pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb2.write.parquet(\"output/pdb_filteredToCaps.parquet\") # this doesn't work -Sean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdb is an MMTFEncoder object: need to turn this into something secondaryStructureExtractorFull expects or modify secondaryStructureExtractorFull to make use of the MMTFEncoder\n",
    "\n",
    "# from mmtfPyspark.utils import mmtfStructure\n",
    "\n",
    "obj = pdb.take(1)[0][1].encode_data()\n",
    "structure = mmtfStructure.MmtfStructure(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'CA', 'C', 'O', 'CB', 'H1', 'HA', 'HB1', 'HB2', 'HB3']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.keys()\n",
    "#structure.group_type_list\n",
    "structure.group_list[0]['atomNameList']\n",
    "#obj['structureId']\n",
    "#obj['entityList']\n",
    "#obj['bioAssemblyList']\n",
    "#obj['atomIdList']\n",
    "#temp = type(pdb.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Torsion angle and secondary structure info ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING\n",
      "MAPPED\n"
     ]
    }
   ],
   "source": [
    "#from mmtfPyspark.datasets import secondaryStructureExtractor\n",
    "\n",
    "data = secondaryStructureExtractorFull.get_dataset(pdb).toPandas()\n",
    "#data = secondaryStructureExtractor.get_dataset(pdb).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbId</th>\n",
       "      <th>chain</th>\n",
       "      <th>resi</th>\n",
       "      <th>resn</th>\n",
       "      <th>phi</th>\n",
       "      <th>psi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>ALA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-60.699390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>VAL</td>\n",
       "      <td>-146.134125</td>\n",
       "      <td>57.770145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>ILE</td>\n",
       "      <td>-111.048912</td>\n",
       "      <td>138.046036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>ASN</td>\n",
       "      <td>-159.172882</td>\n",
       "      <td>43.077713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>THR</td>\n",
       "      <td>-118.766075</td>\n",
       "      <td>165.706207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>PHE</td>\n",
       "      <td>-60.730888</td>\n",
       "      <td>-49.274670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>ASP</td>\n",
       "      <td>-71.140984</td>\n",
       "      <td>-37.861027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>GLY</td>\n",
       "      <td>-69.076538</td>\n",
       "      <td>-48.363979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>8</td>\n",
       "      <td>VAL</td>\n",
       "      <td>-69.851631</td>\n",
       "      <td>-40.401875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1buj</td>\n",
       "      <td>A</td>\n",
       "      <td>9</td>\n",
       "      <td>ALA</td>\n",
       "      <td>-59.562462</td>\n",
       "      <td>-37.430336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbId chain  resi resn         phi         psi\n",
       "0  1buj     A     0  ALA         NaN  -60.699390\n",
       "1  1buj     A     1  VAL -146.134125   57.770145\n",
       "2  1buj     A     2  ILE -111.048912  138.046036\n",
       "3  1buj     A     3  ASN -159.172882   43.077713\n",
       "4  1buj     A     4  THR -118.766075  165.706207\n",
       "5  1buj     A     5  PHE  -60.730888  -49.274670\n",
       "6  1buj     A     6  ASP  -71.140984  -37.861027\n",
       "7  1buj     A     7  GLY  -69.076538  -48.363979\n",
       "8  1buj     A     8  VAL  -69.851631  -40.401875\n",
       "9  1buj     A     9  ALA  -59.562462  -37.430336"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0241172dcc97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pdb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chain\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m                            \u001b[0;31m# num pdbs,    max len of seqs, num features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "groups = data.groupby([\"pdb\", \"chain\"])\n",
    "                           # num pdbs,    max len of seqs, num features\n",
    "training_data = np.empty((groups.ngroups, 5000,            22))\n",
    "truth = np.empty((groups.ngroups, 5000,            2))\n",
    "for i, (pdb, chain), group in enumerate(groups):\n",
    "    for j, aa in enumerate(group):\n",
    "        if j>=5000: break\n",
    "        training_data[i,j,:] = aa[4:]\n",
    "        truth[i,j,:] = is_cap(aa[0], aa[1])\n",
    "\n",
    "def is_cap(pdb, chain, resi):\n",
    "    if capsdb_pdbs.filter(capsdb.pdbId.like(pdb), capsdb.chain.like(chain), resi>=caps.startcap, resi<=caps.endcap)>0:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write features to H5 file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_pdb.write.mode('overwrite').format('hdf').save('./features.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get truth labels and Save to H5 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out truth.h5..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Bidirectional LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_features, num_outputs=2, latent_dim=100):\n",
    "    \"\"\"Create a Seq2Seq Bidirectional LSTM\n",
    "    From: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features : int\n",
    "        The number of features in your trianing data\n",
    "    num_outputs : int\n",
    "        Number of outputs to rpedict, i.e. number of classes or 2 for binary\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A new Keras Seq2Seq Bidirectional LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_features))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, num_outputes))\n",
    "    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_outputs, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    X_data = HDF5Matrix('features.hdf5', 'data')\n",
    "    y_data = HDF5Matrix('truth.h5', 'data')\n",
    "    model = create_model()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Automicatlly determine batch sizes, train/test splits\n",
    "    model.fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mybinder-environment",
   "language": "python",
   "name": "conda_mybinder-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
