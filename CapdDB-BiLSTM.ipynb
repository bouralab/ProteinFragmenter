{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Helix Capping Residues #\n",
    "\n",
    "The goal is to identify residues just before an alpha helix begins or the residues just after the helix ends. This will improve secondary structure predictors becuase they often extend too far or do not start at the right place. \n",
    "\n",
    "The CapsDB has annoted sequences of structures of helix capping residues that can be used to train a deep nueral net. We will use a Bidirectional LSTM using phi/psi features to see if it will those will be good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Features ##\n",
    "### MMTF Pyspark Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.webfilters import Pisces\n",
    "from mmtfPyspark.filters import ContainsLProteinChain\n",
    "from mmtfPyspark.mappers import StructureToPolymerChains\n",
    "from mmtfPyspark.ml import ProteinSequenceEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secondaryStructureExtractorFull\n",
    "#import mmtfToASA\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Context ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"DeepCap\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out chains not in CapsDB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, array_contains\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "capsdb = sqlContext.read.parquet('caps_descriptors.parquet')\n",
    "capsdb_pdbs = capsdb.select(concat(upper(col(\"pdbId\")), lit(\".\"), col(\"chain\")).alias(\"id\")).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsdb_pdbs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import BooleanType\n",
    "\n",
    "# def hasHelixCapInfo(enc_obj, ids):\n",
    "#     return(enc_obj[0] in ids)\n",
    "\n",
    "# def udf_hasHelixCapInfo(ids):\n",
    "#     return udf(lambda t: hasHelixCapInfo(t, ids))\n",
    "#     # I'm not sure how to call these successfully - Sean\n",
    "\n",
    "capsdb_ids = set()\n",
    "[capsdb_ids.add(list[0]) for list in capsdb_pdbs.select(\"id\").collect()]\n",
    "\n",
    "class HasHelixCapInfo(object):\n",
    "    '''This filter returns true if the structure is in CAPSDB'''\n",
    "    def __call__(self, t):\n",
    "        return(t[0] in capsdb_ids)\n",
    "    # This works but I don't like that it refers to a global variable: not sure how to change that right now - Sean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MMTF File and get a set of L-protein chains ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"full\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/full.tar && tar -xvf full.tar\n",
    "        \n",
    "if not os.path.isdir(\"reduced\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/reduced.tar && tar -xvf reduced.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mmtfReader.read_sequence_file('full') \\\n",
    "    .flatMap(StructureToPolymerChains()) \\\n",
    "    .filter(ContainsLProteinChain()) \\\n",
    "    .filter(HasHelixCapInfo())\n",
    "#pdb.count()\n",
    "#pdb.take(2)\n",
    "#?pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Torsion angle and secondary structure info ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mmtfPyspark.datasets import secondaryStructureExtractor\n",
    "\n",
    "data = secondaryStructureExtractorFull.get_dataset(pdb).toPandas()\n",
    "#data = secondaryStructureExtractor.get_dataset(pdb).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read output of above get_dataset operation from parquet file\n",
    "parquetPath = '/home/ec2-user/SageMaker/ProteinFragmenter/datacaps.parquet'\n",
    "dataframe = sqlContext.read.parquet(parquetPath)\n",
    "data = dataframe.toPandas()\n",
    "data = data.drop('__index_level_0__', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = capsdb.toPandas()\n",
    "df = pd.merge(data, df1, left_on=('pdbId','chain'), right_on=('pdbid','chain'), how='inner')\n",
    "df = df[['pdbId', 'chain', 'resi', 'resn', 'phi', 'psi', 'startcap', 'endcap']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_cap'] = df.apply(lambda x: 1 if (x['resi'] >= x['startcap'] and x['resi'] <= x['endcap']) else 0, axis=1)\n",
    "df_caps = df.groupby([\"pdbId\", \"chain\", \"resi\"])['is_cap'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caps = pd.merge(data, df_caps, left_on=('pdbId','chain', 'resi'), right_on=('pdbId','chain', 'resi'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cap(pdbId, chain, resi, is_cap):\n",
    "    if is_cap == 1:\n",
    "        return([1,0])\n",
    "    elif is_cap == 0:\n",
    "        return([0,1])\n",
    "    else:\n",
    "        raise ValueError(\"is_cap must be 0 or 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data_caps.groupby([\"pdbId\", \"chain\"])\n",
    "                           # num pdbs,    max len of seqs, num features\n",
    "training_data = np.zeros((groups.ngroups, 5000, 2), dtype=float)\n",
    "                           # num pdbs,    max len of seqs, length of one-hot encoded target\n",
    "truth = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n",
    "truth_lagged = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, ((pdbid, chain), group) in enumerate(groups):\n",
    "    for j, featuretuple in enumerate(group.itertuples()):\n",
    "        if j>=5000: break\n",
    "        training_data[i,j,:] = (featuretuple.phi, featuretuple.psi)\n",
    "        truth[i,j,:] = is_cap(featuretuple.pdbId, featuretuple.chain, featuretuple.resi, featuretuple.is_cap)\n",
    "        if (j > 0):\n",
    "            truth_lagged[i,j-1,:] = truth[i,j,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data[10]\n",
    "#truth[2]\n",
    "truth[8,:,0].sum()\n",
    "#truth.shape\n",
    "#training_data.shape\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write features, labels to H5 file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "os.remove('features.h5')\n",
    "os.remove('truth.h5')\n",
    "os.remove('truthlag.h5')\n",
    "\n",
    "h5f = h5py.File('features.h5', 'w')\n",
    "h5f.create_dataset('training_data', data=training_data)\n",
    "h5f.close()\n",
    "\n",
    "h5tr = h5py.File('truth.h5', 'w')\n",
    "h5tr.create_dataset('truth', data=truth)\n",
    "h5tr.close()\n",
    "\n",
    "h5trl = h5py.File('truthlag.h5', 'w')\n",
    "h5trl.create_dataset('truthlag', data=truth_lagged)\n",
    "h5trl.close()\n",
    "\n",
    "#os.remove(\"features.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Bidirectional LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(num_features, num_outputs=2, latent_dim=5):\n",
    "    \"\"\"Create a Seq2Seq Bidirectional LSTM\n",
    "    From: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features : int\n",
    "        The number of features in your trianing data\n",
    "    num_outputs : int\n",
    "        Number of outputs to rpedict, i.e. number of classes or 2 for binary\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A new Keras Seq2Seq Bidirectional LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_features))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    #encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    #encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    #state_h = Concatenate()([forward_h, backward_h])\n",
    "    #state_c = Concatenate()([forward_c, backward_c])\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    #decoder_inputs = Input(shape=(None, num_outputs))\n",
    "        \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    #decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    #decoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "    #decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, num_outputs))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    #decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_dense = Dense(num_outputs, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Train model\n",
    "    encoder_input_data = HDF5Matrix('features.h5', 'training_data')\n",
    "    decoder_input_data = HDF5Matrix('truth.h5', 'truth')\n",
    "    decoder_target_data = HDF5Matrix('truthlag.h5', 'truthlag')\n",
    "    \n",
    "    #Automicatlly determine batch sizes, train/test splits\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data, shuffle=\"batch\", epochs=10) #, callbacks=get_callbacks(model))\n",
    "    #print(model.get_weights())\n",
    "    #with open('model_weights.txt', 'w') as f:\n",
    "    #    f.write(print(model.get_weights()))\n",
    "    for (i, obj) in enumerate(model.get_weights()):\n",
    "        obj.tofile(\"weights_layer{0}\".format(str(i)))\n",
    "    #.tofile(\"test2.txt\")\n",
    "    \n",
    "    # Create encoder/decoder model\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_obj, logging_file=\"training.log\", verbosity=1, early_stopping_patience=None):\n",
    "    callbacks = list()\n",
    "    callbacks.append(ModelCheckpoint(model_obj, save_best_only=True))\n",
    "    callbacks.append(CSVLogger(logging_file, append=True))\n",
    "#    callbacks.append(LossHistory('losshistory.log', append=True))\n",
    "    callbacks.append(TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='batch'))\n",
    "#     if learning_rate_epochs:\n",
    "#         callbacks.append(LearningRateScheduler(partial(step_decay, initial_lrate=initial_learning_rate,\n",
    "#                                                        drop=learning_rate_drop, epochs_drop=learning_rate_epochs)))\n",
    "#     else:\n",
    "#         callbacks.append(ReduceLROnPlateau(factor=learning_rate_drop, patience=learning_rate_patience,\n",
    "#                                            verbose=verbosity))\n",
    "#     if early_stopping_patience:\n",
    "#         callbacks.append(EarlyStopping(verbose=verbosity, patience=early_stopping_patience))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data = HDF5Matrix('features.h5', 'training_data')\n",
    "training_data.tofile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of training data, if needed for faster debugging\n",
    "\n",
    "#training_data = HDF5Matrix('features.h5', 'training_data')\n",
    "import h5py\n",
    "hf = h5py.File('full_dataset_features/features.h5', 'r')\n",
    "training_data = hf.get('training_data').value\n",
    "training_data = training_data[0:128,:,:]\n",
    "hf.close()\n",
    "\n",
    "hf = h5py.File('full_dataset_features/truth.h5', 'r')\n",
    "truth = hf.get('truth').value\n",
    "truth = truth[0:128,:,:]\n",
    "hf.close()\n",
    "\n",
    "hf = h5py.File('full_dataset_features/truthlag.h5', 'r')\n",
    "truth_lagged = hf.get('truthlag').value\n",
    "truth_lagged = truth_lagged[0:128,:,:]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import numpy\n",
    "#numpy.set_printoptions(threshold=numpy.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard\n",
    "\n",
    "# class LossHistory(keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.losses = []\n",
    "\n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "\n",
    "#encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model = train()\n",
    "#model = train()\n",
    "encoder_model, decoder_model = create_and_train_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save_weights(\"encoder_weights\")\n",
    "decoder_model.save_weights(\"decoder_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_StS_BiLSTM.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_StS_BiLSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.24711749e+307])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fromfile(\"weights_layer7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.22199549e-006, -1.07067662e-005, -3.37417133e-007,\n",
       "        1.67356665e-005,  6.53985916e-007,  2.24711749e+307,\n",
       "        2.24711749e+307, -5.06000970e-012,  1.29100576e-017,\n",
       "        2.70294593e-006, -4.85684593e-008,  1.20094240e-010,\n",
       "       -1.87995487e-006, -1.16249375e-005, -3.17621879e-006,\n",
       "        2.24711749e+307,  2.24711749e+307, -3.75207243e-009,\n",
       "        1.70528141e-006, -9.35567296e-007])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    print(states_value)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 2))\n",
    "    # Populate the first character of target sequence with the start residue.\n",
    "    target_seq[0, 0, :] = [0, 1] # decoder tokens one-hot encoding hardcoded here\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_chain = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        decoded_chain.append(sampled_token_index)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if len(decoded_chain) > 5000:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        sampled_residue = [0, 0]\n",
    "        sampled_residue[sampled_token_index] = 1\n",
    "        target_seq = np.zeros((1, 1, 2))\n",
    "        target_seq[0, 0, :] = sampled_residue\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Sean: ran til here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/myBinder-environment/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#training_data = HDF5Matrix('features.h5', 'training_data')\n",
    "import h5py\n",
    "hf = h5py.File('features.h5', 'r')\n",
    "df_training = hf.get('training_data').value\n",
    "hf.close()\n",
    "\n",
    "# import h5py\n",
    "# hf = h5py.File('truth.h5', 'r')\n",
    "# df_truth = hf.get('truth').value\n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_training.shape\n",
    "#df_training.shape\n",
    "df_truth[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_training[4:5,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[nan, nan, nan, nan, nan]], dtype=float32), array([[nan, nan, nan, nan, nan]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "result = decode_sequence(temp, encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for j in result:\n",
    "    i += j\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_model.get_layer()\n",
    "#encoder_model.layers\n",
    "#decoder_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mybinder-environment",
   "language": "python",
   "name": "conda_mybinder-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
