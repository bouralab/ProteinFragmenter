{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Helix Capping Residues #\n",
    "\n",
    "The goal is to identify residues just before an alpha helix begins or the residues just after the helix ends. This will improve secondary structure predictors becuase they often extend too far or do not start at the right place. \n",
    "\n",
    "The CapsDB has annoted sequences of structures of helix capping residues that can be used to train a deep nueral net. We will use a Bidirectional LSTM using phi/psi features to see if it will those will be good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Features ##\n",
    "### MMTF Pyspark Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.webfilters import Pisces\n",
    "from mmtfPyspark.filters import ContainsLProteinChain\n",
    "from mmtfPyspark.mappers import StructureToPolymerChains\n",
    "from mmtfPyspark.ml import ProteinSequenceEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/ProteinFragmenter'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import secondaryStructureExtractorFull\n",
    "#import mmtfToASA\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Context ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"DeepCap\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out chains not in CapsDB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, array_contains\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "capsdb = sqlContext.read.parquet('caps_descriptors.parquet')\n",
    "capsdb_pdbs = capsdb.select(concat(upper(col(\"pdbId\")), lit(\".\"), col(\"chain\")).alias(\"id\")).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsdb_pdbs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import BooleanType\n",
    "\n",
    "# def hasHelixCapInfo(enc_obj, ids):\n",
    "#     return(enc_obj[0] in ids)\n",
    "\n",
    "# def udf_hasHelixCapInfo(ids):\n",
    "#     return udf(lambda t: hasHelixCapInfo(t, ids))\n",
    "#     # I'm not sure how to call these successfully - Sean\n",
    "\n",
    "capsdb_ids = set()\n",
    "[capsdb_ids.add(list[0]) for list in capsdb_pdbs.select(\"id\").collect()]\n",
    "\n",
    "class HasHelixCapInfo(object):\n",
    "    '''This filter returns true if the structure is in CAPSDB'''\n",
    "    def __call__(self, t):\n",
    "        return(t[0] in capsdb_ids)\n",
    "    # This works but I don't like that it refers to a global variable: not sure how to change that right now - Sean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MMTF File and get a set of L-protein chains ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"full\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/full.tar && tar -xvf full.tar\n",
    "        \n",
    "if not os.path.isdir(\"reduced\"):\n",
    "    !wget https://mmtf.rcsb.org/v1.0/hadoopfiles/reduced.tar && tar -xvf reduced.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mmtfReader.read_sequence_file('full') \\\n",
    "    .flatMap(StructureToPolymerChains()) \\\n",
    "    .filter(ContainsLProteinChain()) \\\n",
    "    .filter(HasHelixCapInfo())\n",
    "#pdb.count()\n",
    "#pdb.take(2)\n",
    "#?pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Torsion angle and secondary structure info ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mmtfPyspark.datasets import secondaryStructureExtractor\n",
    "\n",
    "data = secondaryStructureExtractorFull.get_dataset(pdb).toPandas()\n",
    "#data = secondaryStructureExtractor.get_dataset(pdb).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read output of above get_dataset operation from parquet file\n",
    "parquetPath = '/home/ec2-user/SageMaker/ProteinFragmenter/datacaps.parquet'\n",
    "dataframe = sqlContext.read.parquet(parquetPath)\n",
    "data = dataframe.toPandas()\n",
    "data = data.drop('__index_level_0__', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = capsdb.toPandas()\n",
    "df = pd.merge(data, df1, left_on=('pdbId','chain'), right_on=('pdbid','chain'), how='inner')\n",
    "df = df[['pdbId', 'chain', 'resi', 'resn', 'phi', 'psi', 'startcap', 'endcap']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_cap'] = df.apply(lambda x: 1 if (x['resi'] >= x['startcap'] and x['resi'] <= x['endcap']) else 0, axis=1)\n",
    "df_caps = df.groupby([\"pdbId\", \"chain\", \"resi\"])['is_cap'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caps = pd.merge(data, df_caps, left_on=('pdbId','chain', 'resi'), right_on=('pdbId','chain', 'resi'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.Polypeptide import aa3\n",
    "one_hot_encoded = pd.DataFrame(data_caps.resn.apply(lambda x: secondaryStructureExtractorFull.get_residue(x)).tolist(), columns=aa3)\n",
    "one_hot_encoded.head()\n",
    "data_caps = data_caps.join(one_hot_encoded)\n",
    "data_caps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = training_data[0:128,:,:]\n",
    "\n",
    "def is_cap(pdbId, chain, resi, is_cap):\n",
    "    if is_cap == 1:\n",
    "        return([1,0])\n",
    "    elif is_cap == 0:\n",
    "        return([0,1])\n",
    "    else:\n",
    "        raise ValueError(\"is_cap must be 0 or 1\")\n",
    "\n",
    "def angle_to_cos(angle):\n",
    "    if(angle == 0 or np.isnan(angle)):\n",
    "        return 0\n",
    "    else:\n",
    "        #print(\"working\")\n",
    "        return np.cos(np.pi * angle/180)\n",
    "\n",
    "def angle_to_sin(angle):\n",
    "    if(angle == 0 or np.isnan(angle)):\n",
    "        return 0\n",
    "    else:\n",
    "        #print(\"working\")\n",
    "        return np.sin(np.pi * angle/180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data_caps.groupby([\"pdbId\", \"chain\"])\n",
    "                           # num pdbs,    max len of seqs, num features\n",
    "training_data = np.zeros((groups.ngroups, 5000, 24), dtype=float)\n",
    "                           # num pdbs,    max len of seqs, length of one-hot encoded target\n",
    "truth = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n",
    "truth_lagged = np.zeros((groups.ngroups, 5000, 2), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, ((pdbid, chain), group) in enumerate(groups):\n",
    "    for j, featuretuple in enumerate(group.itertuples()):\n",
    "        if j>=5000: break\n",
    "        truth[i,j,:] = is_cap(featuretuple.pdbId, featuretuple.chain, featuretuple.resi, featuretuple.is_cap)\n",
    "        training_data[i,j,:] = (angle_to_cos(featuretuple.phi), angle_to_sin(featuretuple.phi), \n",
    "                              angle_to_cos(featuretuple.psi), angle_to_sin(featuretuple.psi), featuretuple.ALA,\n",
    "                              featuretuple.CYS,featuretuple.ASP,featuretuple.GLU,featuretuple.PHE,\n",
    "                              featuretuple.GLY,featuretuple.HIS,featuretuple.ILE,featuretuple.LYS,\n",
    "                              featuretuple.LEU,featuretuple.MET,featuretuple.ASN,featuretuple.PRO,\n",
    "                              featuretuple.GLN,featuretuple.ARG,featuretuple.SER,featuretuple.THR,\n",
    "                              featuretuple.VAL,featuretuple.TRP,featuretuple.TYR)\n",
    "        if (j > 0):\n",
    "            truth_lagged[i,j-1,:] = truth[i,j,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[10,2,:]\n",
    "#truth[2]\n",
    "#truth[8,:,0].sum()\n",
    "#truth.shape\n",
    "training_data.shape\n",
    "#os.getcwd()\n",
    "training_data = training_data[:, 0:100, :]\n",
    "truth = truth[:, 0:100, :]\n",
    "truth_lagged = truth_lagged[:, 0:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of training data, if needed for faster debugging\n",
    "\n",
    "#training_data = HDF5Matrix('features.h5', 'training_data')\n",
    "import h5py\n",
    "hf = h5py.File('full_dataset_features/features.h5', 'r')\n",
    "training_data = hf.get('training_data').value\n",
    "hf.close()\n",
    "\n",
    "hf = h5py.File('full_dataset_features/truth.h5', 'r')\n",
    "truth = hf.get('truth').value\n",
    "hf.close()\n",
    "\n",
    "hf = h5py.File('full_dataset_features/truthlag.h5', 'r')\n",
    "truth_lagged = hf.get('truthlag').value\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write features, labels to H5 file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "os.remove('features.h5')\n",
    "os.remove('truth.h5')\n",
    "os.remove('truthlag.h5')\n",
    "\n",
    "h5f = h5py.File('features.h5', 'w')\n",
    "h5f.create_dataset('training_data', data=training_data)\n",
    "h5f.close()\n",
    "\n",
    "h5tr = h5py.File('truth.h5', 'w')\n",
    "h5tr.create_dataset('truth', data=truth)\n",
    "h5tr.close()\n",
    "\n",
    "h5trl = h5py.File('truthlag.h5', 'w')\n",
    "h5trl.create_dataset('truthlag', data=truth_lagged)\n",
    "h5trl.close()\n",
    "\n",
    "#os.remove(\"features.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate Spark ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Bidirectional LSTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(num_features, num_outputs=2, latent_dim=100):\n",
    "    \"\"\"Create a Seq2Seq Bidirectional LSTM\n",
    "    From: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_features : int\n",
    "        The number of features in your trianing data\n",
    "    num_outputs : int\n",
    "        Number of outputs to rpedict, i.e. number of classes or 2 for binary\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A new Keras Seq2Seq Bidirectional LSTM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_features), name=\"input\")\n",
    "    #mask = Masking(mask_value=0.)(encoder_inputs) # Masking is not supported by CuDNNLSTM\n",
    "    encoder = CuDNNLSTM(latent_dim, return_state=True, name=\"encoder_LSTM\") #, activation='relu')\n",
    "    #keras.layers.add([encoder, bn])\n",
    "    \n",
    "    #encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    #bn = BatchNormalization()(encoder_outputs)\n",
    "    #encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    #state_h = Concatenate()([forward_h, backward_h])\n",
    "    #state_c = Concatenate()([forward_c, backward_c])\n",
    "        \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    #decoder_inputs = Input(shape=(None, num_outputs))\n",
    "        \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    #decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    #decoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True))\n",
    "    #decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None, num_outputs), name=\"decoder_input\")\n",
    "    decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_LSTM\") #, activation='relu')\n",
    "    #decoder_lstm = LSTM(latent_dim * 2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    #bn_decoder = BatchNormalization()(decoder_outputs)\n",
    "    \n",
    "    decoder_dense = Dense(num_outputs, activation='softmax', name=\"decoder_dense\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"model1\")\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # summarize layers\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Train model\n",
    "    encoder_input_data = HDF5Matrix('features.h5', 'training_data')\n",
    "    decoder_input_data = HDF5Matrix('truth.h5', 'truth')\n",
    "    decoder_target_data = HDF5Matrix('truthlag.h5', 'truthlag')\n",
    "    \n",
    "    #Automicatlly determine batch sizes, train/test splits\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data, shuffle=\"batch\", epochs=2, callbacks=get_callbacks(model))\n",
    "    for (i, obj) in enumerate(model.get_weights()):\n",
    "        obj.tofile(\"weights_layer{0}\".format(str(i)))\n",
    "    \n",
    "    # Create encoder/decoder model\n",
    "    encoder_model = Model(encoder_inputs, encoder_states, name=\"encoder_model\")\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states, name=\"decoder_model\")\n",
    "    #decoder_model.add(BatchNormalization())\n",
    "    \n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(num_features, num_outputs=2, latent_dim=100):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(input_shape=(None, None, 20), return_sequences=True))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_obj, logging_file=\"training.log\", verbosity=1, early_stopping_patience=None):\n",
    "    callbacks = list()\n",
    "    #callbacks.append(ModelCheckpoint(model_obj, save_best_only=True))\n",
    "    #callbacks.append(CSVLogger(logging_file, append=True))\n",
    "#    callbacks.append(LossHistory('losshistory.log', append=True))\n",
    "    callbacks.append(TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='batch'))\n",
    "#     if learning_rate_epochs:\n",
    "#         callbacks.append(LearningRateScheduler(partial(step_decay, initial_lrate=initial_learning_rate,\n",
    "#                                                        drop=learning_rate_drop, epochs_drop=learning_rate_epochs)))\n",
    "#     else:\n",
    "#         callbacks.append(ReduceLROnPlateau(factor=learning_rate_drop, patience=learning_rate_patience,\n",
    "#                                            verbose=verbosity))\n",
    "#     if early_stopping_patience:\n",
    "#         callbacks.append(EarlyStopping(verbose=verbosity, patience=early_stopping_patience))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data = HDF5Matrix('features.h5', 'training_data')\n",
    "training_data.tofile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subset of training data, if needed for faster debugging\n",
    "\n",
    "#training_data = HDF5Matrix('features.h5', 'training_data')\n",
    "import h5py\n",
    "hf = h5py.File('full_dataset_features/features.h5', 'r')\n",
    "training_data = hf.get('training_data').value\n",
    "#training_data = training_data[0:128,:,:]\n",
    "hf.close()\n",
    "\n",
    "# hf = h5py.File('full_dataset_features/truth.h5', 'r')\n",
    "# truth = hf.get('truth').value\n",
    "# truth = truth[0:128,:,:]\n",
    "# hf.close()\n",
    "\n",
    "# hf = h5py.File('full_dataset_features/truthlag.h5', 'r')\n",
    "# truth_lagged = hf.get('truthlag').value\n",
    "# truth_lagged = truth_lagged[0:128,:,:]\n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data.shape\n",
    "training_data = training_data[:, :, 0:20]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, None, 24)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 2)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_LSTM (CuDNNLSTM)        [(None, 100), (None, 50400       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_LSTM (CuDNNLSTM)        [(None, None, 100),  41600       decoder_input[0][0]              \n",
      "                                                                 encoder_LSTM[0][1]               \n",
      "                                                                 encoder_LSTM[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 2)      202         decoder_LSTM[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 92,202\n",
      "Trainable params: 92,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "6714/6714 [==============================] - 10s 2ms/step - loss: 0.2541 - acc: 0.9059\n",
      "Epoch 2/2\n",
      "6714/6714 [==============================] - 10s 1ms/step - loss: 0.1869 - acc: 0.9363\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM, Concatenate, CuDNNLSTM, Masking\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "# class LossHistory(keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.losses = []\n",
    "\n",
    "#     def on_batch_end(self, batch, logs={}):\n",
    "#         self.losses.append(logs.get('loss'))\n",
    "\n",
    "#encoder_inputs, encoder_states, decoder_inputs, decoder_lstm, decoder_dense, model = train()\n",
    "#model = train()\n",
    "encoder_model, decoder_model = create_and_train_model(num_features=24, num_outputs=2, latent_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save_weights(\"encoder_weights\")\n",
    "decoder_model.save_weights(\"decoder_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_StS_BiLSTM.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_StS_BiLSTM.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.28577856e-14,  3.19465971e-15,  9.97968656e-14,  4.11448434e-17,\n",
       "        7.68643938e-15,  3.28545861e-20,  1.43525735e-16,  3.25894612e-15,\n",
       "        6.64276896e-13,  1.20964505e-11,  1.95491603e-24,  2.82222701e-19,\n",
       "        4.18349289e-12,  2.25621473e-12,  5.15593900e-12, -2.91185474e-20,\n",
       "        3.34469049e-13, -1.07186738e-16,  9.40922985e-15,  7.12299570e-12,\n",
       "       -1.06134464e-14,  2.32598686e-16,  3.59302917e-13,  4.71931264e-15,\n",
       "        3.12599889e-16,  6.31237666e-17,  2.68510950e-12,  5.75789805e-16,\n",
       "       -4.96457838e-20,  3.12619676e-12,  1.04576860e-10, -2.59939417e-23,\n",
       "       -2.35683691e-20, -2.15801210e-19, -1.82597776e-16,  9.29035621e-10,\n",
       "        5.87092109e-13, -2.42708724e-21,  1.18500782e-11,  4.03220301e-19,\n",
       "        6.26595052e-16,  5.10256625e-11,  4.57002807e-18,  5.64523793e-13,\n",
       "        5.08412797e-17,  1.53720384e-14,  1.34543882e-14,  5.30674925e-19,\n",
       "        9.63889773e-15, -1.24110392e-21, -5.37286959e-17, -4.06572748e-22,\n",
       "       -5.71220422e-19,  1.81368149e-16, -7.99613765e-25,  2.47452064e-23,\n",
       "        1.24420064e-20,  2.38146909e-18,  1.35378501e-22, -1.38368765e-18,\n",
       "       -1.38633530e-19,  2.79534259e-19, -1.05380029e-14, -2.91286672e-20,\n",
       "       -1.08303558e-15, -1.70191126e-16, -6.60265623e-25, -4.55691216e-17,\n",
       "       -2.52904342e-15, -1.04221620e-16, -2.92538229e-16,  1.08315098e-23,\n",
       "        7.03645401e-16, -1.43881639e-22,  7.30614379e-18,  5.98065005e-22,\n",
       "       -1.33118981e-18,  1.40723602e-23, -8.08543696e-22, -5.12355602e-17,\n",
       "       -6.24242014e-15, -7.46450834e-29, -1.39120921e-18, -4.97881348e-19,\n",
       "       -9.67134877e-21, -8.69923049e-12, -1.35288542e-29, -6.28824868e-19,\n",
       "       -2.54381430e-17,  4.40431795e-19,  1.27033018e-20, -4.12108427e-14,\n",
       "       -7.41230798e-27,  1.52611077e-24,  2.85843302e-18, -2.29228976e-16,\n",
       "       -5.45318731e-18, -1.48956848e-19, -2.58613895e-25,  1.11366883e-21,\n",
       "        5.74208199e-19,  3.42110606e-23, -7.61048640e-17,  9.72492893e-34,\n",
       "        1.07207549e-18, -3.05964709e-24,  2.06165538e-19, -6.93606738e-18,\n",
       "       -5.99847903e-19, -2.69725107e-18,  1.23596576e-19, -6.19896692e-19,\n",
       "        6.21749368e-19,  3.19993889e-21,  5.00010305e-19, -2.39917746e-17,\n",
       "        1.19351744e-19, -8.90379021e-18,  2.81055134e-19,  5.60322630e-18,\n",
       "       -1.14249639e-17,  2.83970372e-17,  9.41179123e-15, -1.43845078e-19,\n",
       "       -1.34816757e-17,  1.93642799e-19,  3.71131832e-20, -3.44281130e-18,\n",
       "        6.88355741e-17, -2.39139998e-17, -4.47560228e-17,  1.10567349e-18,\n",
       "        4.78715291e-17, -8.82602322e-19,  8.19506618e-17,  1.95450919e-26,\n",
       "        1.16243297e-18, -1.07378196e-18,  1.66330531e-21, -3.09229848e-18,\n",
       "       -1.65672385e-18,  1.96288867e-21,  2.30068047e-21,  2.56933080e-21,\n",
       "        6.00221520e-18,  2.79709281e-16,  4.03333461e-17, -1.26553359e-18,\n",
       "       -6.70811564e-21,  3.27137056e-19,  2.45137195e-14,  1.96544719e-15,\n",
       "        2.41328586e-17,  7.50428939e-16,  5.60157877e-15,  3.86233694e-20,\n",
       "        1.27619828e-14,  1.29507234e-16,  3.03760974e-13,  7.30833612e-12,\n",
       "       -5.52399584e-19,  5.61607853e-17,  4.85476150e-12,  2.19214272e-12,\n",
       "        2.79075242e-12, -2.89448123e-19,  1.81205988e-13, -4.92112177e-17,\n",
       "        2.31305596e-15,  6.06528970e-12, -8.98617414e-15,  3.05816818e-18,\n",
       "        4.36387644e-14,  5.06996734e-15,  6.43735211e-19,  4.95016705e-17,\n",
       "        2.91050038e-12,  4.83528027e-17, -1.21165895e-17,  2.61148349e-12,\n",
       "        4.97742469e-11, -4.21019742e-20, -5.08505705e-17, -7.49305774e-22,\n",
       "       -5.67619959e-16,  1.37532051e-09,  1.73050619e-13,  7.72565785e-22,\n",
       "        6.51379382e-12,  3.06448026e-19,  1.12598918e-17,  7.37747217e-11,\n",
       "        1.28572900e-17,  2.30659294e-13,  1.93491151e-18,  1.54102249e-15,\n",
       "        8.49414296e-17,  4.30087059e-25,  7.17287310e-15, -1.75697999e-18,\n",
       "        1.28577856e-14,  3.19465971e-15,  9.97968656e-14,  4.11448434e-17,\n",
       "        7.68643938e-15,  3.28545861e-20,  1.43525735e-16,  3.25894612e-15,\n",
       "        6.64276896e-13,  1.20964505e-11,  1.95491603e-24,  2.82222701e-19,\n",
       "        4.18349289e-12,  2.25621473e-12,  5.15593900e-12, -2.91185474e-20,\n",
       "        3.34469049e-13, -1.07186738e-16,  9.40922985e-15,  7.12299570e-12,\n",
       "       -1.06134464e-14,  2.32598686e-16,  3.59302917e-13,  4.71931264e-15,\n",
       "        3.12599889e-16,  6.31237666e-17,  2.68510950e-12,  5.75789805e-16,\n",
       "       -4.96457838e-20,  3.12619676e-12,  1.04576860e-10, -2.59939417e-23,\n",
       "       -2.35683691e-20, -2.15801210e-19, -1.82597776e-16,  9.29035621e-10,\n",
       "        5.87092109e-13, -2.42708724e-21,  1.18500782e-11,  4.03220301e-19,\n",
       "        6.26595052e-16,  5.10256625e-11,  4.57002807e-18,  5.64523793e-13,\n",
       "        5.08412797e-17,  1.53720384e-14,  1.34543882e-14,  5.30674925e-19,\n",
       "        9.63889773e-15, -1.24110392e-21,  6.72169867e-03,  7.56958219e-03,\n",
       "        7.18281883e-03,  9.11052710e-03,  7.69854429e-03,  7.99360311e-03,\n",
       "        8.20422357e-03,  8.56764799e-03,  8.03039408e-03,  7.10478145e-03,\n",
       "        7.30719510e-03,  8.37949848e-03,  5.74146676e-03,  7.38295633e-03,\n",
       "        6.19879830e-03,  6.52672815e-03,  7.70109239e-03,  6.75755832e-03,\n",
       "        6.05182815e-03,  6.60678837e-03,  6.43096212e-03,  7.97479039e-03,\n",
       "        9.34866256e-03,  7.59342033e-03,  8.67019779e-03,  8.06920415e-03,\n",
       "        7.10847694e-03,  7.98134691e-03,  7.54062179e-03,  6.73265848e-03,\n",
       "        5.88904414e-03,  7.77638796e-03,  7.10427108e-03,  7.19313788e-03,\n",
       "        7.43327382e-03,  3.37024452e-03,  7.78376403e-03,  7.17468652e-03,\n",
       "        6.84614199e-03,  8.42381453e-03,  8.20540076e-03,  5.26124331e-03,\n",
       "        7.75038916e-03,  7.93342477e-03,  8.58445650e-03,  6.46577868e-03,\n",
       "        6.98416772e-03,  7.30140600e-03,  7.71141890e-03,  8.09681600e-03,\n",
       "        5.74208199e-19,  3.42110606e-23, -7.61048640e-17,  9.72492893e-34,\n",
       "        1.07207549e-18, -3.05964709e-24,  2.06165538e-19, -6.93606738e-18,\n",
       "       -5.99847903e-19, -2.69725107e-18,  1.23596576e-19, -6.19896692e-19,\n",
       "        6.21749368e-19,  3.19993889e-21,  5.00010305e-19, -2.39917746e-17,\n",
       "        1.19351744e-19, -8.90379021e-18,  2.81055134e-19,  5.60322630e-18,\n",
       "       -1.14249639e-17,  2.83970372e-17,  9.41179123e-15, -1.43845078e-19,\n",
       "       -1.34816757e-17,  1.93642799e-19,  3.71131832e-20, -3.44281130e-18,\n",
       "        6.88355741e-17, -2.39139998e-17, -4.47560228e-17,  1.10567349e-18,\n",
       "        4.78715291e-17, -8.82602322e-19,  8.19506618e-17,  1.95450919e-26,\n",
       "        1.16243297e-18, -1.07378196e-18,  1.66330531e-21, -3.09229848e-18,\n",
       "       -1.65672385e-18,  1.96288867e-21,  2.30068047e-21,  2.56933080e-21,\n",
       "        6.00221520e-18,  2.79709281e-16,  4.03333461e-17, -1.26553359e-18,\n",
       "       -6.70811564e-21,  3.27137056e-19,  2.45137195e-14,  1.96544719e-15,\n",
       "        2.41328586e-17,  7.50428939e-16,  5.60157877e-15,  3.86233694e-20,\n",
       "        1.27619828e-14,  1.29507234e-16,  3.03760974e-13,  7.30833612e-12,\n",
       "       -5.52399584e-19,  5.61607853e-17,  4.85476150e-12,  2.19214272e-12,\n",
       "        2.79075242e-12, -2.89448123e-19,  1.81205988e-13, -4.92112177e-17,\n",
       "        2.31305596e-15,  6.06528970e-12, -8.98617414e-15,  3.05816818e-18,\n",
       "        4.36387644e-14,  5.06996734e-15,  6.43735211e-19,  4.95016705e-17,\n",
       "        2.91050038e-12,  4.83528027e-17, -1.21165895e-17,  2.61148349e-12,\n",
       "        4.97742469e-11, -4.21019742e-20, -5.08505705e-17, -7.49305774e-22,\n",
       "       -5.67619959e-16,  1.37532051e-09,  1.73050619e-13,  7.72565785e-22,\n",
       "        6.51379382e-12,  3.06448026e-19,  1.12598918e-17,  7.37747217e-11,\n",
       "        1.28572900e-17,  2.30659294e-13,  1.93491151e-18,  1.54102249e-15,\n",
       "        8.49414296e-17,  4.30087059e-25,  7.17287310e-15, -1.75697999e-18])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fromfile(\"weights_layer5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    print(states_value)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 2))\n",
    "    # Populate the first character of target sequence with the start residue.\n",
    "    target_seq[0, 0, :] = [0, 1] # decoder tokens one-hot encoding hardcoded here\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_chain = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        print(output_tokens)\n",
    "        sampled_token_index = (np.argmax(output_tokens[0, -1, :]) + 1) % 2\n",
    "        decoded_chain.append(sampled_token_index) # flip indices\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if len(decoded_chain) >= 100:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        sampled_residue = [0, 0]\n",
    "        sampled_residue[sampled_token_index] = 1\n",
    "        target_seq = np.zeros((1, 1, 2))\n",
    "        target_seq[0, 0, :] = sampled_residue\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/ProteinFragmenter'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Sean: ran til here\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data = HDF5Matrix('features.h5', 'training_data')\n",
    "import h5py\n",
    "hf = h5py.File('features.h5', 'r')\n",
    "df_training = hf.get('training_data').value\n",
    "hf.close()\n",
    "\n",
    "# import h5py\n",
    "# hf = h5py.File('truth.h5', 'r')\n",
    "# df_truth = hf.get('truth').value\n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df_training[2:3,:,:]\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-4.61753607e-01, -3.99733692e-01, -6.83733076e-02,\n",
      "         4.30623330e-02, -4.01364863e-01, -5.75511251e-03,\n",
      "         5.07989749e-02, -4.91346531e-02, -2.35608732e-03,\n",
      "         1.25614271e-01, -4.13277894e-01,  2.05996484e-01,\n",
      "        -5.96140146e-01,  4.68936674e-02, -4.25173163e-01,\n",
      "        -2.83412814e-01,  5.58889151e-01,  1.20824836e-01,\n",
      "         5.44418514e-01, -7.51006156e-02, -1.09569833e-01,\n",
      "         4.84094024e-01,  4.51451361e-01, -1.29060462e-01,\n",
      "        -4.38907176e-01,  6.12171710e-01, -2.31075868e-01,\n",
      "        -9.26003605e-02, -9.31617245e-02,  4.83249575e-01,\n",
      "        -9.59240198e-02, -1.62749693e-01, -5.22550717e-02,\n",
      "         1.24774754e-01, -4.74804193e-01, -6.47274703e-02,\n",
      "        -5.88638037e-02, -2.35303538e-03,  1.16782390e-01,\n",
      "         2.42176414e-01, -2.90354133e-01,  1.19337127e-01,\n",
      "        -2.20247507e-01, -4.46341336e-01, -3.66005637e-02,\n",
      "        -5.36113679e-01,  3.13704252e-01,  2.12985486e-01,\n",
      "         2.06312418e-01, -3.54501675e-03, -4.06374276e-01,\n",
      "        -5.85928500e-01,  1.40186235e-01, -6.03802741e-01,\n",
      "        -1.03378883e-02, -3.36036205e-01, -7.79013336e-02,\n",
      "        -4.28101063e-01, -1.03651628e-01,  6.61478145e-04,\n",
      "         6.04087472e-01, -4.90631945e-02, -2.82529771e-01,\n",
      "        -3.83274965e-02,  4.80166852e-01,  1.74485847e-01,\n",
      "        -1.71497196e-01,  2.55837709e-01, -2.40236387e-01,\n",
      "        -1.36574551e-01,  3.44479471e-01, -1.60744846e-01,\n",
      "        -6.48293868e-02, -3.17962766e-01,  2.13650130e-02,\n",
      "         3.50332618e-01,  2.21861526e-01,  4.27666157e-01,\n",
      "        -6.46229833e-02, -3.90598625e-01,  6.99953586e-02,\n",
      "         1.41783394e-02, -7.65584648e-01,  3.08551550e-01,\n",
      "         3.45622361e-01, -1.10862121e-01, -3.25100385e-02,\n",
      "        -9.67113897e-02,  2.54145831e-01, -3.62451822e-01,\n",
      "         2.73069710e-01, -4.20621455e-01,  5.32876194e-01,\n",
      "         3.69598605e-02,  6.56213015e-02,  7.15762824e-02,\n",
      "        -1.35375410e-01,  2.57578164e-01, -3.83814096e-01,\n",
      "         9.98110231e-03]], dtype=float32), array([[-1.50349140e+00, -9.53114092e-01, -1.49648041e-01,\n",
      "         8.37381259e-02, -1.15562928e+00, -1.87396947e-02,\n",
      "         1.21667430e-01, -1.00086227e-01, -5.15844347e-03,\n",
      "         3.54708940e-01, -9.12849724e-01,  5.17889440e-01,\n",
      "        -1.77595901e+00,  1.05165854e-01, -1.16550231e+00,\n",
      "        -7.50462770e-01,  2.28178740e+00,  2.74497479e-01,\n",
      "         1.63662326e+00, -1.64194733e-01, -2.26187140e-01,\n",
      "         3.00938368e+00,  1.42185700e+00, -2.69446969e-01,\n",
      "        -1.22701716e+00,  2.29236674e+00, -5.44339597e-01,\n",
      "        -2.37215340e-01, -2.19592214e-01,  1.74116540e+00,\n",
      "        -2.30982110e-01, -4.12417442e-01, -1.08147003e-01,\n",
      "         3.38165760e-01, -1.80345047e+00, -1.23037137e-01,\n",
      "        -1.35567278e-01, -6.52007898e-03,  2.67254084e-01,\n",
      "         5.04347086e-01, -7.79756069e-01,  2.56895065e-01,\n",
      "        -5.68968832e-01, -9.63431776e-01, -1.14983909e-01,\n",
      "        -2.63670206e+00,  7.59639204e-01,  5.56545377e-01,\n",
      "         5.68762720e-01, -6.57478161e-03, -2.92229247e+00,\n",
      "        -2.09670877e+00,  2.50666618e-01, -2.61098933e+00,\n",
      "        -2.17106100e-02, -1.29085720e+00, -2.46494338e-01,\n",
      "        -1.85152888e+00, -3.03661376e-01,  1.43784366e-03,\n",
      "         3.92984962e+00, -1.05986200e-01, -6.66330159e-01,\n",
      "        -1.11167803e-01,  2.29420424e+00,  3.90156120e-01,\n",
      "        -3.64628166e-01,  6.63724005e-01, -4.96179402e-01,\n",
      "        -3.16991508e-01,  7.47364461e-01, -3.81860524e-01,\n",
      "        -1.98513165e-01, -2.06668353e+00,  4.52014767e-02,\n",
      "         8.53689432e-01,  5.44871271e-01,  9.29693758e-01,\n",
      "        -1.39112920e-01, -1.11856842e+00,  1.69366270e-01,\n",
      "         3.97657976e-02, -2.50161886e+00,  4.83162075e-01,\n",
      "         1.28794324e+00, -3.06049436e-01, -6.80330619e-02,\n",
      "        -1.48974791e-01,  5.36139071e-01, -1.10000432e+00,\n",
      "         5.34117043e-01, -1.47219014e+00,  2.63684082e+00,\n",
      "         8.15956295e-02,  1.66540325e-01,  1.46155387e-01,\n",
      "        -3.21718872e-01,  6.34604156e-01, -8.87550890e-01,\n",
      "         1.88499130e-02]], dtype=float32)]\n",
      "[[[0.00480297 0.995197  ]]]\n",
      "[[[0.8203279 0.1796721]]]\n",
      "[[[0.01097792 0.989022  ]]]\n",
      "[[[0.86957884 0.13042115]]]\n",
      "[[[0.00953973 0.9904603 ]]]\n",
      "[[[0.89061505 0.10938498]]]\n",
      "[[[0.01145814 0.98854184]]]\n",
      "[[[0.9095879  0.09041214]]]\n",
      "[[[0.01369277 0.9863072 ]]]\n",
      "[[[0.9138088  0.08619118]]]\n",
      "[[[0.0149866 0.9850134]]]\n",
      "[[[0.91303635 0.08696368]]]\n",
      "[[[0.01579325 0.9842068 ]]]\n",
      "[[[0.9115913  0.08840872]]]\n",
      "[[[0.01640379 0.9835962 ]]]\n",
      "[[[0.9104646 0.0895354]]]\n",
      "[[[0.01687997 0.98311996]]]\n",
      "[[[0.9097863  0.09021367]]]\n",
      "[[[0.01722371 0.9827763 ]]]\n",
      "[[[0.9094665  0.09053354]]]\n",
      "[[[0.01743782 0.9825622 ]]]\n",
      "[[[0.9093788  0.09062118]]]\n",
      "[[[0.01753983 0.98246014]]]\n",
      "[[[0.909421   0.09057894]]]\n",
      "[[[0.01755823 0.9824417 ]]]\n",
      "[[[0.90952486 0.09047513]]]\n",
      "[[[0.01752428 0.98247576]]]\n",
      "[[[0.9096491  0.09035097]]]\n",
      "[[[0.01746524 0.9825348 ]]]\n",
      "[[[0.9097699  0.09023014]]]\n",
      "[[[0.01740093 0.982599  ]]]\n",
      "[[[0.9098742  0.09012575]]]\n",
      "[[[0.01734331 0.98265666]]]\n",
      "[[[0.90995586 0.09004416]]]\n",
      "[[[0.01729786 0.98270214]]]\n",
      "[[[0.9100132  0.08998683]]]\n",
      "[[[0.01726569 0.98273426]]]\n",
      "[[[0.9100483  0.08995171]]]\n",
      "[[[0.01724537 0.98275465]]]\n",
      "[[[0.91006553 0.08993447]]]\n",
      "[[[0.01723439 0.9827656 ]]]\n",
      "[[[0.9100698  0.08993018]]]\n",
      "[[[0.01722999 0.98276997]]]\n",
      "[[[0.9100662  0.08993383]]]\n",
      "[[[0.01722975 0.9827702 ]]]\n",
      "[[[0.9100586  0.08994137]]]\n",
      "[[[0.01723172 0.98276824]]]\n",
      "[[[0.9100502  0.08994984]]]\n",
      "[[[0.01723455 0.9827655 ]]]\n",
      "[[[0.9100425  0.08995748]]]\n",
      "[[[0.01723736 0.9827626 ]]]\n",
      "[[[0.9100368  0.08996327]]]\n",
      "[[[0.01723969 0.9827603 ]]]\n",
      "[[[0.9100329  0.08996705]]]\n",
      "[[[0.01724136 0.9827587 ]]]\n",
      "[[[0.91003096 0.08996902]]]\n",
      "[[[0.01724237 0.9827576 ]]]\n",
      "[[[0.91003036 0.08996963]]]\n",
      "[[[0.01724282 0.9827572 ]]]\n",
      "[[[0.9100308  0.08996924]]]\n",
      "[[[0.01724287 0.9827571 ]]]\n",
      "[[[0.9100316  0.08996833]]]\n",
      "[[[0.01724269 0.98275733]]]\n",
      "[[[0.91003275 0.08996724]]]\n",
      "[[[0.01724236 0.9827576 ]]]\n",
      "[[[0.9100338  0.08996615]]]\n",
      "[[[0.01724201 0.982758  ]]]\n",
      "[[[0.9100347  0.08996525]]]\n",
      "[[[0.01724168 0.98275834]]]\n",
      "[[[0.9100355  0.08996449]]]\n",
      "[[[0.0172414 0.9827586]]]\n",
      "[[[0.91003597 0.089964  ]]]\n",
      "[[[0.01724121 0.9827588 ]]]\n",
      "[[[0.91003627 0.08996369]]]\n",
      "[[[0.01724107 0.98275894]]]\n",
      "[[[0.9100365 0.0899635]]]\n",
      "[[[0.01724099 0.98275906]]]\n",
      "[[[0.91003656 0.08996346]]]\n",
      "[[[0.01724095 0.98275906]]]\n",
      "[[[0.9100365  0.08996347]]]\n",
      "[[[0.01724094 0.98275906]]]\n",
      "[[[0.9100365 0.0899635]]]\n",
      "[[[0.01724094 0.98275906]]]\n",
      "[[[0.9100365  0.08996356]]]\n",
      "[[[0.01724095 0.98275906]]]\n",
      "[[[0.9100364  0.08996359]]]\n",
      "[[[0.01724098 0.98275906]]]\n",
      "[[[0.9100364  0.08996361]]]\n",
      "[[[0.01724098 0.98275906]]]\n",
      "[[[0.9100364  0.08996366]]]\n",
      "[[[0.01724099 0.98275906]]]\n",
      "[[[0.9100364  0.08996366]]]\n",
      "[[[0.017241   0.98275906]]]\n",
      "[[[0.9100364  0.08996364]]]\n",
      "[[[0.017241   0.98275906]]]\n",
      "[[[0.91003627 0.08996367]]]\n",
      "[[[0.017241   0.98275906]]]\n",
      "[[[0.9100364  0.08996366]]]\n",
      "[[[0.017241   0.98275906]]]\n",
      "[[[0.9100364  0.08996364]]]\n"
     ]
    }
   ],
   "source": [
    "result = decode_sequence(temp, encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for j in result:\n",
    "    i += j\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mybinder-environment",
   "language": "python",
   "name": "conda_mybinder-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
