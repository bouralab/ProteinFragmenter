{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Helix Capping Residues #\n",
    "\n",
    "The goal is to identify residues just before an alpha helix begins or the residues just after the helix ends. This will improve secondary structure predictors becuase they often extend too far or do not start at the right place. \n",
    "\n",
    "The CapsDB has annoted sequences of structures of helix capping residues that can be used to train a deep nueral net. We will use a Bidirectional LSTM using phi/psi features to see if it will those will be good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Features ##\n",
    "### MMTF Pyspark Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from mmtfPyspark.io import mmtfReader\n",
    "from mmtfPyspark.webfilters import Pisces\n",
    "from mmtfPyspark.filters import ContainsLProteinChain\n",
    "from mmtfPyspark.mappers import StructureToPolymerChains\n",
    "from mmtfPyspark.ml import ProteinSequenceEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secondaryStructureExtractorFull\n",
    "#import mmtfToASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark Context ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[8]\").appName(\"DeepCap\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQLContext ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, array_contains\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in filtered cap+MMTF data from parquet file###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read output of above get_dataset operation from parquet file\n",
    "parquetPath = '/home/ec2-user/SageMaker/ProteinFragmenter/datacaps.parquet'\n",
    "dataframe = sqlContext.read.parquet(parquetPath)\n",
    "data = dataframe.toPandas()\n",
    "data = data.drop('__index_level_0__', axis=1)\n",
    "\n",
    "capsdb = sqlContext.read.parquet('caps_descriptors.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Torsion angle and secondary structure info ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head(10)\n",
    "#\", \".join(data.pdbId.unique())\n",
    "#temp = data[data.pdbId == '7odc']#.resn.shape\n",
    "#temp.sort()\n",
    "#temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = capsdb.toPandas()\n",
    "df = pd.merge(data, df1, left_on=('pdbId','chain'), right_on=('pdbid','chain'), how='inner')\n",
    "df = df[['pdbId', 'chain', 'resi', 'resn', 'phi', 'psi', 'startcap', 'endcap']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_cap'] = df.apply(lambda x: 1 if (x['resi'] >= x['startcap'] and x['resi'] <= x['endcap']) else 0, axis=1)\n",
    "df_caps = df.groupby([\"pdbId\", \"chain\", \"resi\"])['is_cap'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caps = pd.merge(data, df_caps, left_on=('pdbId','chain', 'resi'), right_on=('pdbId','chain', 'resi'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbId</th>\n",
       "      <th>chain</th>\n",
       "      <th>resi</th>\n",
       "      <th>resn</th>\n",
       "      <th>phi</th>\n",
       "      <th>psi</th>\n",
       "      <th>is_cap</th>\n",
       "      <th>ALA</th>\n",
       "      <th>CYS</th>\n",
       "      <th>ASP</th>\n",
       "      <th>...</th>\n",
       "      <th>MET</th>\n",
       "      <th>ASN</th>\n",
       "      <th>PRO</th>\n",
       "      <th>GLN</th>\n",
       "      <th>ARG</th>\n",
       "      <th>SER</th>\n",
       "      <th>THR</th>\n",
       "      <th>VAL</th>\n",
       "      <th>TRP</th>\n",
       "      <th>TYR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>THR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163.677383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>GLY</td>\n",
       "      <td>-66.660973</td>\n",
       "      <td>160.703186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>SER</td>\n",
       "      <td>-123.853607</td>\n",
       "      <td>-7.871733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>LEU</td>\n",
       "      <td>-74.896896</td>\n",
       "      <td>137.483932</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>TYR</td>\n",
       "      <td>-134.419830</td>\n",
       "      <td>140.864288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbId chain  resi resn         phi         psi  is_cap  ALA  CYS  ASP ...   \\\n",
       "0  2ygn     A     1  THR         NaN  163.677383       0    0    0    0 ...    \n",
       "1  2ygn     A     2  GLY  -66.660973  160.703186       0    0    0    0 ...    \n",
       "2  2ygn     A     3  SER -123.853607   -7.871733       0    0    0    0 ...    \n",
       "3  2ygn     A     4  LEU  -74.896896  137.483932       0    0    0    0 ...    \n",
       "4  2ygn     A     5  TYR -134.419830  140.864288       0    0    0    0 ...    \n",
       "\n",
       "   MET  ASN  PRO  GLN  ARG  SER  THR  VAL  TRP  TYR  \n",
       "0    0    0    0    0    0    0    1    0    0    0  \n",
       "1    0    0    0    0    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    1    0    0    0    0  \n",
       "3    0    0    0    0    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0    0    0    0    1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio.PDB.Polypeptide import aa3\n",
    "one_hot_encoded = pd.DataFrame(data_caps.resn.apply(lambda x: secondaryStructureExtractorFull.get_residue(x)).tolist(), columns=aa3)\n",
    "one_hot_encoded.head()\n",
    "data_caps = data_caps.join(one_hot_encoded)\n",
    "data_caps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbId</th>\n",
       "      <th>chain</th>\n",
       "      <th>resi</th>\n",
       "      <th>resn</th>\n",
       "      <th>phi</th>\n",
       "      <th>psi</th>\n",
       "      <th>is_cap</th>\n",
       "      <th>ALA</th>\n",
       "      <th>CYS</th>\n",
       "      <th>ASP</th>\n",
       "      <th>...</th>\n",
       "      <th>MET</th>\n",
       "      <th>ASN</th>\n",
       "      <th>PRO</th>\n",
       "      <th>GLN</th>\n",
       "      <th>ARG</th>\n",
       "      <th>SER</th>\n",
       "      <th>THR</th>\n",
       "      <th>VAL</th>\n",
       "      <th>TRP</th>\n",
       "      <th>TYR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>THR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163.677383</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>GLY</td>\n",
       "      <td>-66.660973</td>\n",
       "      <td>160.703186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>SER</td>\n",
       "      <td>-123.853607</td>\n",
       "      <td>-7.871733</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>LEU</td>\n",
       "      <td>-74.896896</td>\n",
       "      <td>137.483932</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2ygn</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>TYR</td>\n",
       "      <td>-134.419830</td>\n",
       "      <td>140.864288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbId chain  resi resn         phi         psi  is_cap  ALA  CYS  ASP ...   \\\n",
       "0  2ygn     A     1  THR         NaN  163.677383       0    0    0    0 ...    \n",
       "1  2ygn     A     2  GLY  -66.660973  160.703186       0    0    0    0 ...    \n",
       "2  2ygn     A     3  SER -123.853607   -7.871733       0    0    0    0 ...    \n",
       "3  2ygn     A     4  LEU  -74.896896  137.483932       0    0    0    0 ...    \n",
       "4  2ygn     A     5  TYR -134.419830  140.864288       0    0    0    0 ...    \n",
       "\n",
       "   MET  ASN  PRO  GLN  ARG  SER  THR  VAL  TRP  TYR  \n",
       "0    0    0    0    0    0    0    1    0    0    0  \n",
       "1    0    0    0    0    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    1    0    0    0    0  \n",
       "3    0    0    0    0    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0    0    0    0    1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_caps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_cap(pdbId, chain, resi, is_cap):\n",
    "    if is_cap == 1:\n",
    "        return(1)\n",
    "    elif is_cap == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        raise ValueError(\"is_cap must be 0 or 1\")\n",
    "\n",
    "def angle_to_cos(angle):\n",
    "    if(angle == 0 or np.isnan(angle)):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.cos(np.pi * angle/180)\n",
    "\n",
    "def angle_to_sin(angle):\n",
    "    if(angle == 0 or np.isnan(angle)):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sin(np.pi * angle/180)\n",
    "    \n",
    "def parse_feature_file(filename):\n",
    "    # Read file\n",
    "    df = pd.read_csv(filename, sep=\"\\t\", skiprows=range(10))\n",
    "    df = df.reset_index()\n",
    "    # Set column names\n",
    "    cnames=[\n",
    "    \"RESIDUE_CLASS1_IS_HYDROPHOBIC\",\n",
    "    \"RESIDUE_CLASS1_IS_CHARGED\",\n",
    "    \"RESIDUE_CLASS1_IS_POLAR\",\n",
    "    \"RESIDUE_CLASS1_IS_UNKNOWN\",\n",
    "    \"RESIDUE_CLASS2_IS_NONPOLAR\",\n",
    "    \"RESIDUE_CLASS2_IS_POLAR\",\n",
    "    \"RESIDUE_CLASS2_IS_BASIC\",\n",
    "    \"RESIDUE_CLASS2_IS_ACIDIC\",\n",
    "    \"RESIDUE_CLASS2_IS_UNKNOWN\",\n",
    "    ]\n",
    "    l = [[\"{}_SHELL{}\".format(c, i) for c in cnames] for i in range(6)]\n",
    "    cnames = [item for sublist in l for item in sublist]\n",
    "    cnames.insert(0, \"env\")\n",
    "    cnames.extend([\"hash\", \"x\", \"y\", \"z\", \"VERBOSITY\", \"location\"])\n",
    "    df.columns = cnames\n",
    "    \n",
    "    df = df.drop([\"hash\", \"x\", \"y\", \"z\", \"VERBOSITY\"], axis=1)\n",
    "    \n",
    "    # filter out non-AA structures\n",
    "    aminoAcids = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS',\n",
    "    'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', \n",
    "    'TYR', 'VAL']\n",
    "    df['residue'] = df.location.str[:3]\n",
    "    df = df[df.residue.isin(aminoAcids)]\n",
    "    \n",
    "    # Split identifiers\n",
    "\n",
    "    # split residue number\n",
    "    df_residue = df.location.str[3:].str.split(\":\", 1, expand=True)\n",
    "    df_residue.columns = [\"ordernum\", \"chainatom\"]\n",
    "    df = pd.merge(df, df_residue, left_index=True, right_index=True)\n",
    "\n",
    "    # split chain\n",
    "    df_chain = df.chainatom.str.split(\"@\", 1, expand=True)\n",
    "    df_chain.columns = [\"chain\", \"atom\"]\n",
    "    df = pd.merge(df, df_chain, left_index=True, right_index=True)\n",
    "    \n",
    "    df['pdbId'] = df.env.str[4:8]\n",
    "\n",
    "    df = df.drop([\"env\", \"location\", \"chainatom\"], axis=1)\n",
    "    \n",
    "    #df.ordernum = df.ordernum.astype(int)\n",
    "\n",
    "    # Aggregate to residue level\n",
    "    groups = df.groupby(['chain', 'ordernum', 'residue'], sort=False)\n",
    "    groupnums = groups.ngroup(ascending=True)\n",
    "    df_agg_max = groups.max().reset_index()\n",
    "    df_agg_max = df_agg_max.drop(['atom', 'ordernum'], axis=1)\n",
    "    df_agg_max['resi'] = df_agg_max.index+1\n",
    "    return(df_agg_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse feature files to get additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"feature/feature-3.1.0/feature_files\")\n",
    "filelist = []\n",
    "f = open(\"feature_parsing.log\", \"w\")\n",
    "for i, filename in enumerate(files):\n",
    "    print(i, filename)\n",
    "    f.write(\"File {}: protein {}\\n\".format(i, filename))\n",
    "    df = parse_feature_file(\"feature/feature-3.1.0/feature_files/{}\".format(filename))\n",
    "    filelist.append(df)\n",
    "\n",
    "f.close()\n",
    "feature_df = pd.concat(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv(\"Feature_vectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_caps\n",
    "data_caps2 = data_caps.merge(feature_df, left_on=[\"pdbId\", \"chain\", \"resi\"], right_on=[\"pdbId\", \"chain\", \"resi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122, 78)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_caps2.columns\n",
    "#data_caps.shape\n",
    "#feature_df.shape\n",
    "#data_caps[data_caps.pdbId == \"2ygn\"]\n",
    "#feature_df[feature_df.pdbId == \"2ygn\"]\n",
    "#data_caps.groupby(['pdbId', 'chain']).ngroup().unique().shape\n",
    "#feature_df.groupby(['pdbId', 'chain']).ngroup().unique().shape\n",
    "#data_caps2.groupby(['pdbId', 'chain']).ngroup().unique().shape\n",
    "train_chains[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data into list of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1270\n"
     ]
    }
   ],
   "source": [
    "groups = data_caps2.groupby([\"pdbId\", \"chain\"])\n",
    "                           # num pdbs,    max len of seqs, num features\n",
    "\n",
    "# Check max length of protein chains\n",
    "# maxlen = 0\n",
    "# for i, ((pdbid, chain), group) in enumerate(groups):\n",
    "#     l = 0\n",
    "#     for j, featuretuple in enumerate(group.itertuples()):\n",
    "#         l += 1\n",
    "#         if l > maxlen:\n",
    "#             maxlen = l\n",
    "# print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chains = []\n",
    "label_chains = []\n",
    "laglabel_chains = []\n",
    "\n",
    "for i, ((pdbid, chain), group) in enumerate(groups):\n",
    "    # Create empty arrays\n",
    "    train_chain = np.zeros((1300,78), dtype=float) # max chain length is 1288 residues\n",
    "    label_chain = np.zeros((1300,1), dtype=int)\n",
    "    laglabel_chain = np.zeros((5000,1), dtype=int)\n",
    "    \n",
    "    # Populate arrays\n",
    "    for j, featuretuple in enumerate(group.itertuples()):\n",
    "        train_chain[j, :] = (angle_to_cos(featuretuple.phi), \n",
    "                             angle_to_sin(featuretuple.phi), \n",
    "                             angle_to_cos(featuretuple.psi), \n",
    "                             angle_to_sin(featuretuple.psi),\n",
    "                            featuretuple.ALA,\n",
    "                            featuretuple.CYS,\n",
    "                            featuretuple.ASP,\n",
    "                            featuretuple.GLU,\n",
    "                            featuretuple.PHE,\n",
    "                            featuretuple.GLY,\n",
    "                            featuretuple.HIS,\n",
    "                            featuretuple.ILE,\n",
    "                            featuretuple.LYS,\n",
    "                            featuretuple.LEU,\n",
    "                            featuretuple.MET,\n",
    "                            featuretuple.ASN,\n",
    "                            featuretuple.PRO,\n",
    "                            featuretuple.GLN,\n",
    "                            featuretuple.ARG,\n",
    "                            featuretuple.SER,\n",
    "                            featuretuple.THR,\n",
    "                            featuretuple.VAL,\n",
    "                            featuretuple.TRP,\n",
    "                            featuretuple.TYR,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_HYDROPHOBIC_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_CHARGED_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_POLAR_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_UNKNOWN_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_NONPOLAR_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_POLAR_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_BASIC_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_ACIDIC_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_UNKNOWN_SHELL0,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_HYDROPHOBIC_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_CHARGED_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_POLAR_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_UNKNOWN_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_NONPOLAR_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_POLAR_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_BASIC_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_ACIDIC_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_UNKNOWN_SHELL1,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_HYDROPHOBIC_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_CHARGED_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_POLAR_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_UNKNOWN_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_NONPOLAR_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_POLAR_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_BASIC_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_ACIDIC_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_UNKNOWN_SHELL2,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_HYDROPHOBIC_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_CHARGED_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_POLAR_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_UNKNOWN_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_NONPOLAR_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_POLAR_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_BASIC_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_ACIDIC_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_UNKNOWN_SHELL3,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_HYDROPHOBIC_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_CHARGED_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_POLAR_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_UNKNOWN_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_NONPOLAR_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_POLAR_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_BASIC_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_ACIDIC_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_UNKNOWN_SHELL4,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_HYDROPHOBIC_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_CHARGED_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_POLAR_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS1_IS_UNKNOWN_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_NONPOLAR_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_POLAR_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_BASIC_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_ACIDIC_SHELL5,\n",
    "                            featuretuple.RESIDUE_CLASS2_IS_UNKNOWN_SHELL5)\n",
    "        label_chain[j,0] = is_cap(featuretuple.pdbId, featuretuple.chain, featuretuple.resi, featuretuple.is_cap)\n",
    "        if (j > 0):\n",
    "            laglabel_chain[j-1,0] = label_chain[j,0]\n",
    "    \n",
    "    # Trim zeros\n",
    "    trimmed_train = train_chain[~np.all(train_chain == 0, axis=1)]\n",
    "    trimmed_label = label_chain[:trimmed_train.shape[0]]\n",
    "    trimmed_laglabel = label_chain[:trimmed_train.shape[0]+1]\n",
    "    \n",
    "    # Add chain data to lists of arrays\n",
    "    train_chains.append(trimmed_train)\n",
    "    label_chains.append(trimmed_label)\n",
    "    laglabel_chains.append(trimmed_laglabel)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write training data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"pickled_data/train_chains_78.pickle\",\"wb\")\n",
    "pickle.dump(train_chains, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"pickled_data/label_chains_78.pickle\",\"wb\")\n",
    "pickle.dump(label_chains, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"pickled_data/laglabel_chains_78.pickle\",\"wb\")\n",
    "pickle.dump(laglabel_chains, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below reads in 1-dim (binary) labels and writes back out as 2-dim labels (one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "label_chain_in = open(\"pickled_data/label_chains_78.pickle\",\"rb\")\n",
    "labels = pickle.load(label_chain_in)\n",
    "\n",
    "newlabels = []\n",
    "for i, l in enumerate(labels):\n",
    "    temp = np.zeros([l.shape[0], 2], dtype=int)\n",
    "    temp[:,1] = l[:,0]\n",
    "    temp[:,0] = (l[:,0]+1)%2\n",
    "    newlabels.append(temp)\n",
    "\n",
    "pickle_out = open(\"pickled_data/label_chains_78.pickle\",\"wb\")\n",
    "pickle.dump(newlabels, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "laglabel_chain_in = open(\"pickled_data/laglabel_chains_78.pickle\",\"rb\")\n",
    "labelslag = pickle.load(laglabel_chain_in)\n",
    "\n",
    "newlabelslag = []\n",
    "for i, l in enumerate(labelslag):\n",
    "    temp = np.zeros([l.shape[0], 2], dtype=int)\n",
    "    temp[:,1] = l[:,0]\n",
    "    temp[:,0] = (l[:,0]+1)%2\n",
    "    newlabelslag.append(temp)\n",
    "\n",
    "pickle_out = open(\"pickled_data/laglabel_chains_78.pickle\",\"wb\")\n",
    "pickle.dump(newlabelslag, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below reads in train/label and writes out lists sorted by chain length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_chain_in = open(\"pickled_data/train_chains.pickle\",\"rb\")\n",
    "train = pickle.load(train_chain_in)\n",
    "\n",
    "lens = [len(chain) for chain in train]\n",
    "inds = range(len(train))\n",
    "lenSeries = pd.Series(data=lens, index=inds).sort_values()\n",
    "newInds = lenSeries.index.values\n",
    "newlist = []\n",
    "[newlist.append(train[i]) for i in newInds]\n",
    "\n",
    "pickle_out = open(\"pickled_data/train_chains_sorted.pickle\",\"wb\")\n",
    "pickle.dump(newlist, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# now sort label list\n",
    "label_chain_in = open(\"pickled_data/label_chains.pickle\",\"rb\")\n",
    "labels = pickle.load(label_chain_in)\n",
    "\n",
    "newlist2 = []\n",
    "[newlist2.append(labels[i]) for i in newInds]\n",
    "\n",
    "pickle_out = open(\"pickled_data/label_chains_sorted.pickle\",\"wb\")\n",
    "pickle.dump(newlist2, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# now sort laglabel list\n",
    "laglabel_chain_in = open(\"pickled_data/laglabel_chains.pickle\",\"rb\")\n",
    "labelslag = pickle.load(laglabel_chain_in)\n",
    "\n",
    "newlist3 = []\n",
    "[newlist3.append(labelslag[i]) for i in newInds]\n",
    "\n",
    "pickle_out = open(\"pickled_data/laglabel_chains_sorted.pickle\",\"wb\")\n",
    "pickle.dump(newlist3, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mybinder-environment",
   "language": "python",
   "name": "conda_mybinder-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
