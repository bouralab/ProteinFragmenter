{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Helix Capping Residues #\n",
    "\n",
    "The goal is to identify residues just before an alpha helix begins or the residues just after the helix ends. This will improve secondary structure predictors becuase they often extend too far or do not start at the right place. \n",
    "\n",
    "The CapsDB has annoted sequences of structures of helix capping residues that can be used to train a deep nueral net. We will use a Bidirectional LSTM using phi/psi features to see if it will those will be good predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "from keras.layers import *# Input, Dense, Bidirectional, LSTM, Concatenate, CuDNNLSTM, Masking\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build generator for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(frac_train = 0.75, sort=False):\n",
    "    'Splits data into train and validation sets'\n",
    "    \n",
    "    data_chain_in = open(\"pickled_data/train_chains.pickle\",\"rb\")\n",
    "    data = pickle.load(data_chain_in)\n",
    "    label_chain_in = open(\"pickled_data/label_chains.pickle\",\"rb\")\n",
    "    labels = pickle.load(label_chain_in)\n",
    "    laglabel_chain_in = open(\"pickled_data/laglabel_chains.pickle\",\"rb\")\n",
    "    labelslag = pickle.load(laglabel_chain_in)\n",
    "\n",
    "    n_obs = len(labels)\n",
    "    n_train = np.floor(n_obs*frac_train)\n",
    "    list_IDs = np.arange(n_obs) # length of training set size\n",
    "    np.random.shuffle(list_IDs)\n",
    "    train_IDs = list_IDs[0:int(n_train)].astype(int)\n",
    "    test_IDs = list_IDs[int(n_train):].astype(int)\n",
    "\n",
    "    train_data = [data[i] for i in train_IDs]\n",
    "    train_labels = [labels[i] for i in train_IDs]\n",
    "    train_labelslag = [labelslag[i] for i in train_IDs]\n",
    "    \n",
    "    test_data = [data[i] for i in test_IDs]\n",
    "    test_labels = [labels[i] for i in test_IDs]\n",
    "    test_labelslag = [labelslag[i] for i in test_IDs]\n",
    "    \n",
    "    for typ in [\"train\", \"test\"]:\n",
    "        for dataset in [\"data\", \"labels\", \"labelslag\"]:\n",
    "            name = \"{}_{}\".format(typ, dataset)\n",
    "            datalist = locals()[name]\n",
    "            pickle_out = open(\"pickled_data/staged_data/{}.pickle\".format(name),\"wb\")\n",
    "            pickle.dump(datalist, pickle_out)\n",
    "            pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, partition, batch_size=1, num_features=24, shuffle=True):\n",
    "        'Initialization'\n",
    "        \n",
    "        data_path = \"pickled_data/staged_data/{}_data.pickle\".format(partition)\n",
    "        label_path = \"pickled_data/staged_data/{}_labels.pickle\".format(partition)\n",
    "        data_chain_in = open(data_path,\"rb\")\n",
    "        self.data = pickle.load(data_chain_in)\n",
    "        label_chain_in = open(label_path,\"rb\")\n",
    "        self.labels = pickle.load(label_chain_in)\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.dim = [None,num_features]\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = np.arange(len(self.labels)) # length of training set size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim)\n",
    "        # Initialization\n",
    "        \n",
    "        # hardcode in sizes to test instead of using \"*self.dim\"\n",
    "\n",
    "        # make IDs < 10 to intentially overfit on small data set\n",
    "        #list_IDs_temp = [ID % 10 for ID in list_IDs_temp] \n",
    "        \n",
    "        residues = len(self.labels[list_IDs_temp[0]]) # this only works for batch_size=1\n",
    "        \n",
    "        X = np.empty((self.batch_size, residues, self.num_features))\n",
    "        y = np.empty((self.batch_size, residues, 2), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = self.data[ID]\n",
    "#             print(ID)\n",
    "#             print(y.shape)\n",
    "#             print(self.labels[ID].shape)\n",
    "            \n",
    "            y[i,] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad/batch data generator\n",
    "class DataGeneratorBatch(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, batch_size=1, num_features=24, shuffle=True):\n",
    "        'Initialization'\n",
    "        \n",
    "        # read in data\n",
    "        data_chain_in = open(\"pickled_data/train_chains_sorted.pickle\",\"rb\")\n",
    "        self.train_data = pickle.load(data_chain_in)\n",
    "        label_chain_in = open(\"pickled_data/label_chains_sorted.pickle\",\"rb\")\n",
    "        self.labels = pickle.load(label_chain_in)\n",
    "        \n",
    "        # set self vars\n",
    "        self.num_features = num_features\n",
    "        self.dim = [None,num_features]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # split lists of arrays into batch chunks\n",
    "        n_batch = len(self.train_data) // batch_size\n",
    "        self.train_batch_list = [self.train_data[i:i + batch_size] for i in range(0, n_batch*batch_size, batch_size)]\n",
    "        self.label_batch_list = [self.labels[i:i + batch_size] for i in range(0, n_batch*batch_size, batch_size)]\n",
    "        \n",
    "        # add padding within batches and add to new list of 3D arrays\n",
    "        self.train_batched = []\n",
    "        for i, l in enumerate(self.train_batch_list):\n",
    "            max_len = np.max([len(arr) for arr in l])\n",
    "            batch_array = np.zeros((batch_size, max_len, num_features), dtype=float)\n",
    "            for j, arr in enumerate(l):\n",
    "                batch_array[j,:(arr.shape[0]),:] = arr\n",
    "            self.train_batched.append(batch_array)\n",
    "            \n",
    "        # add padding within batches and add to new list of 3D arrays\n",
    "        self.labels_batched = []\n",
    "        for i, l in enumerate(self.label_batch_list):\n",
    "            max_len = np.max([len(arr) for arr in l])\n",
    "            batch_array = np.zeros((batch_size, max_len, 2), dtype=int)\n",
    "            for j, arr in enumerate(l):\n",
    "                batch_array[j,:(arr.shape[0]),:] = arr\n",
    "            self.labels_batched.append(batch_array)\n",
    "            \n",
    "        self.list_IDs = np.arange(len(self.labels_batched)) # number of batches\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Find list of IDs\n",
    "        list_ID_temp = self.indexes[index]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_ID_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch_index):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim)\n",
    "        # Initialization\n",
    "        \n",
    "        # hardcode in sizes to test instead of using \"*self.dim\"\n",
    "\n",
    "        X = self.train_batch_list[batch_index]\n",
    "        y = self.label_batch_list[batch_index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/test partition files on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LSTM Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_accuracy = []\n",
    "        self.val_balanced_accuracy_score = []\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        metvals = eval_model(self.model, partition=\"test\", metric=\"all\", verbose=False)\n",
    "        \n",
    "        self.val_accuracy.append(metvals[0])\n",
    "        self.val_balanced_accuracy_score.append(metvals[1])\n",
    "        self.val_f1s.append(metvals[2])\n",
    "        \n",
    "        print(\" — accuracy: {} — bal_accuracy: {} — f1: {}\".format(metvals))\n",
    "        return\n",
    "\n",
    "#metrics = Metrics()\n",
    "\n",
    "def get_callbacks(logging_file=\"training3.log\", verbosity=1, early_stopping_patience=None):\n",
    "    callbacks = list()\n",
    "    #callbacks.append(ModelCheckpoint(model_obj, save_best_only=True))\n",
    "    #callbacks.append(CSVLogger(logging_file, append=True))\n",
    "    #callbacks.append(LossHistory('losshistory.log', append=True))\n",
    "    callbacks.append(Metrics())\n",
    "    callbacks.append(TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=1, write_graph=False, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch'))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(batch_size, num_features):\n",
    "\n",
    "    #Xchain = tf.placeholder(tf.float32, [None, batch_size, num_features], name='InputSequence')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(CuDNNLSTM(units=100, # dimensionality of the output space, independent of # timesteps\n",
    "                        return_sequences=True),input_shape=(None, num_features)))\n",
    "    # model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    # if use_dropout:\n",
    "    #     model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(2)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "num_features=24\n",
    "\n",
    "model = create_model(batch_size, num_features)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pickled_data/staged_data/{}_labels.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-319f6acccbe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Instantiate generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#training_generator = DataGeneratorBatch()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-d1d431067c15>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, partition, batch_size, num_features, shuffle)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdata_chain_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pickled_data/staged_data/{}_data.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_chain_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mlabel_chain_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pickled_data/staged_data/{}_labels.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_chain_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickled_data/staged_data/{}_labels.pickle'"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "\n",
    "# Instantiate generator\n",
    "training_generator = DataGenerator(partition=\"train\")\n",
    "test_generator = DataGenerator(partition=\"test\")\n",
    "#training_generator = DataGeneratorBatch()\n",
    "\n",
    "# Train model\n",
    "model.fit_generator(generator=training_generator, epochs=150, callbacks=get_callbacks(), validation_data=test_generator)\n",
    "\n",
    "model.save('batchedBiLSTM_split2.h5') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "model = load_model('batchedBiLSTM_split.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "from keras.models import *\n",
    "\n",
    "# Instantiate generator\n",
    "test_generator = DataGeneratorTest()\n",
    "model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "#for i, samp in enumerate(test_generator):\n",
    "#    print(i, (samp[0].shape), (samp[1].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, partition=\"test\", metric=\"accuracy\", verbose=True):\n",
    "    'Returns and optionally prints the selected metric calculated on partition of data.'\n",
    "    \n",
    "    data_path = \"pickled_data/staged_data/{}_data.pickle\".format(partition)\n",
    "    label_path = \"pickled_data/staged_data/{}_labels.pickle\".format(partition)\n",
    "    data_chain_in = open(data_path,\"rb\")\n",
    "    label_chain_in = open(label_path,\"rb\")\n",
    "    data = pickle.load(data_chain_in)\n",
    "    labels = pickle.load(label_chain_in)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Calculating {} metric on {} samples\".format(metric, len(labels)))\n",
    "        \n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "\n",
    "    # Get predictions and concatenate into array\n",
    "    for i in range(len(labels)):\n",
    "        chain = data[i]\n",
    "        temp = np.empty((1,chain.shape[0],24), dtype=float)\n",
    "        temp[0,:,:] = data[i]\n",
    "\n",
    "        pred = np.argmax(model.predict(temp), axis=2)[0]\n",
    "        truth = np.argmax(labels[i], axis=1)\n",
    "        pred_list.append(pred)\n",
    "        label_list.append(truth)\n",
    "\n",
    "    pred_array = np.concatenate(pred_list)\n",
    "    label_array = np.concatenate(label_list)\n",
    "   \n",
    "    if metric == \"accuracy\":\n",
    "        metval = accuracy_score(label_array, pred_array)\n",
    "    elif metric == \"balanced\":\n",
    "        metval = balanced_accuracy_score(label_array, pred_array)\n",
    "    elif metric == \"f1\":\n",
    "        metval = f1_score(label_array, pred_array)\n",
    "    elif metric == \"all\":\n",
    "        metval = [accuracy_score(label_array, pred_array),\n",
    "                 balanced_accuracy_score(label_array, pred_array),\n",
    "                 f1_score(label_array, pred_array)]\n",
    "        if verbose:\n",
    "            print(metval)\n",
    "    else:\n",
    "        return(\"bad metric argument: use \"\"accuracy\"\",\"\"balanced\"\" or \"\"f1\"\"\")\n",
    "\n",
    "    if (verbose and metric != \"all\"):\n",
    "        print(\"{} on {} set is: {}\".format(metric, partition, metval))\n",
    "    return(metval)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating f1 metric on 1343 samples\n",
      "f1 on test set is: 0.6415008062381169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6415008062381169"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate metric on whole test set\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "eval_model(model, partition=\"test\", metric=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in for eval\n",
    "data_chain_in = open(\"pickled_data/train_chains_sorted.pickle\",\"rb\")\n",
    "train = pickle.load(data_chain_in)\n",
    "label_chain_in = open(\"pickled_data/label_chains_sorted.pickle\",\"rb\")\n",
    "labels = pickle.load(label_chain_in)\n",
    "data_chain_in.close()\n",
    "label_chain_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-242b40412ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutate on specific protein chain\n",
    "index = 1008\n",
    "length = train[index].shape[0]\n",
    "temp = np.empty((1,length,24), dtype=float)\n",
    "temp[0,:,:] = train[index]\n",
    "\n",
    "print(\"Predicted caps\")\n",
    "print(np.argmax(model.predict(temp), axis=2)[0])\n",
    "\n",
    "print(\"True caps\")\n",
    "print(np.argmax(labels[index], axis=1))\n",
    "\n",
    "temp1 = np.argmax(model.predict(temp), axis=2)[0]\n",
    "temp2 = np.argmax(labels[index], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(temp2, temp1)\n",
    "#model.predict_generator(DataGeneratorTest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mybinder-environment",
   "language": "python",
   "name": "conda_mybinder-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
